% !TEX root = ../sommaire.tex

\chapter{Fondamentaux du Deep Learning}

Ce premier chapitre d'annexe fournit les bases du deep learning nécessaires à la compréhension de notre travail, sans supposer de connaissances préalables approfondies.

\section{Histoire et évolutions majeures}

\subsection{Les origines : perceptrons et réseaux multicouches}

L'histoire de l'intelligence artificielle et des réseaux de neurones débute bien avant l'ère moderne du deep learning. Dès les années 1940, Warren McCulloch et Walter Pitts proposent le premier modèle mathématique du neurone artificiel, posant ainsi les fondements théoriques de ce qui deviendra plus tard les réseaux de neurones. Leur modèle, bien que simplifié, capturait l'essence du fonctionnement neuronal : un neurone reçoit des signaux, les combine, et décide de s'activer ou non selon un seuil.

Deux décennies plus tard, en 1958, Frank Rosenblatt fait passer la théorie à la pratique en construisant le Mark I Perceptron, la première implémentation matérielle d'un réseau de neurones. Cette machine, capable d'apprendre à reconnaître des formes simples, suscite un enthousiasme considérable. Rosenblatt va jusqu'à prédire que les perceptrons apprendront bientôt à marcher, parler, voir et même développer une conscience. Malheureusement, ces espoirs seront rapidement tempérés.

En 1969, Marvin Minsky et Seymour Papert publient leur ouvrage "Perceptrons", démontrant mathématiquement les limitations fondamentales de ces systèmes. Le perceptron simple ne peut résoudre le problème du XOR, une fonction logique pourtant élémentaire. Cette découverte, bien que théoriquement correcte, provoque un gel des financements de recherche en réseaux de neurones pour près de deux décennies, période connue sous le nom de "premier hiver de l'IA".

Le salut viendra dans les années 1980 avec l'introduction des perceptrons multicouches (MLP). L'algorithme de rétropropagation du gradient, redécouvert et popularisé par David Rumelhart, Geoffrey Hinton et Ronald Williams en 1986, permet enfin d'entraîner des réseaux comportant plusieurs couches de neurones. Cette percée résout le problème du XOR et ouvre la voie aux réseaux profonds. Pourtant, malgré ce progrès théorique, les limitations computationnelles de l'époque empêchent encore l'exploitation du plein potentiel de ces architectures.

\subsection{Premier hiver de l'IA et renaissance}

Les années 1990 sont marquées par un second hiver de l'IA. Les réseaux de neurones, bien que théoriquement puissants, se heurtent à de nombreux obstacles pratiques. Le problème du vanishing gradient rend l'entraînement de réseaux profonds extrêmement difficile : les gradients deviennent exponentiellement petits à mesure qu'ils se propagent vers les couches initiales, rendant leur apprentissage quasi-impossible. De plus, les ordinateurs de l'époque ne possèdent pas la puissance de calcul nécessaire pour entraîner des modèles complexes sur des données volumineuses.

La renaissance commence discrètement à la fin des années 1990. Yann LeCun et ses collègues aux Bell Labs développent LeNet-5, un réseau de neurones convolutif capable de lire des chèques bancaires avec une précision remarquable. Ce système est déployé commercialement et traite des millions de transactions, prouvant que les réseaux de neurones peuvent fonctionner en conditions réelles. Cependant, l'impact de cette réussite reste limité à quelques domaines d'application spécifiques.

Parallèlement, trois évolutions majeures préparent le terrain pour la révolution à venir. Premièrement, les GPUs (processeurs graphiques) deviennent suffisamment puissants et programmables pour accélérer massivement les calculs matriciels nécessaires aux réseaux de neurones. Deuxièmement, l'ère d'Internet génère des quantités de données sans précédent. Troisièmement, des datasets annotés massifs comme ImageNet, contenant des millions d'images étiquetées, deviennent disponibles pour la communauté scientifique.

\subsection{Révolution AlexNet (2012)}

L'année 2012 marque un tournant historique dans l'histoire du deep learning. Alex Krizhevsky, alors doctorant sous la direction de Geoffrey Hinton, soumet son réseau AlexNet~\cite{Krizhevsky2012} au concours ImageNet Large Scale Visual Recognition Challenge (ILSVRC). Les résultats sont stupéfiants : AlexNet atteint une erreur de 15.3\%, réduisant de moitié l'erreur des méthodes traditionnelles qui stagnaient autour de 25\%. Cette marge d'amélioration, sans précédent dans l'histoire de la vision par ordinateur, fait l'effet d'un électrochoc dans la communauté scientifique.

Le succès d'AlexNet ne repose pas sur une seule innovation, mais sur une combinaison judicieuse d'ingrédients. L'architecture comporte huit couches, ce qui est profond pour l'époque, et mobilise 60 millions de paramètres. Krizhevsky remplace les fonctions d'activation traditionnelles (sigmoïde et tangente hyperbolique) par la fonction ReLU (Rectified Linear Unit), résolvant en grande partie le problème du vanishing gradient. Il introduit le dropout, une technique de régularisation aléatoire qui force le réseau à développer des représentations robustes. L'augmentation intensive des données (rotations, translations, modifications colorimétriques) multiplie artificiellement la taille du dataset d'entraînement. Enfin, et c'est peut-être l'élément le plus crucial, AlexNet est entraîné sur deux GPUs NVIDIA GTX 580, réduisant le temps d'entraînement de plusieurs semaines à quelques jours.

Cette victoire déclenche une course mondiale aux architectures de réseaux de neurones profonds. Les géants technologiques (Google, Facebook, Microsoft, Baidu) investissent massivement dans le domaine. Les publications scientifiques explosent. Le deep learning passe du statut de technique marginale à celui de paradigme dominant en intelligence artificielle.

\subsection{Ère des architectures très profondes}

Si AlexNet prouve que la profondeur est bénéfique, les années suivantes explorent systématiquement cette direction. En 2014, l'équipe Visual Geometry Group (VGG) d'Oxford démontre qu'empiler de nombreux petits filtres convolutifs 3×3 est plus efficace que d'utiliser des grands filtres. VGGNet, avec ses 16 à 19 couches, améliore encore les performances d'ImageNet. Cependant, cette approche révèle rapidement ses limites : le nombre de paramètres explose (138 millions pour VGG-16), la mémoire requise devient prohibitive, et surtout, l'entraînement de réseaux encore plus profonds devient instable.

C'est en 2015 que Kaiming He et ses collègues chez Microsoft Research réalisent une percée fondamentale avec ResNet~\cite{He2016}. Leur intuition est élégante : plutôt que de forcer chaque couche à apprendre directement la fonction désirée $H(x)$, pourquoi ne pas lui demander d'apprendre simplement le résidu $F(x) = H(x) - x$ ? Cette idée se concrétise par les "skip connections" ou connexions résiduelles, qui court-circuitent une ou plusieurs couches. Cette architecture permet d'entraîner des réseaux de 50, 101, et même 152 couches sans dégradation des performances. ResNet remporte ImageNet 2015 avec une erreur de seulement 3.57\%, surpassant les performances humaines estimées à environ 5\%.

Le principe des connexions résiduelles s'avère extraordinairement général et sera réutilisé dans presque toutes les architectures suivantes. DenseNet pousse le concept encore plus loin en connectant chaque couche à toutes les couches précédentes. D'autres travaux se concentrent sur l'efficacité : MobileNet et EfficientNet optimisent le rapport performance/coût computationnel, rendant le deep learning accessible sur smartphones et objets connectés.

\subsection{Révolution Transformer (2017)}

Pendant que la vision par ordinateur connaît ses révolutions successives, le traitement du langage naturel suit sa propre trajectoire. Traditionnellement, les réseaux récurrents (RNN, LSTM, GRU) dominent ce domaine grâce à leur capacité à traiter des séquences de longueur variable. Cependant, ces architectures souffrent de limitations majeures : leur nature séquentielle les rend difficiles à paralléliser, et malgré les améliorations apportées par les LSTM, elles peinent à capturer les dépendances à très longue distance dans le texte.

En 2017, l'article "Attention is All You Need"~\cite{Vaswani2017} de Vaswani et al. introduit le Transformer, une architecture qui abandonne complètement la récurrence au profit d'un mécanisme d'attention. L'idée centrale est révolutionnairement simple : pour comprendre un mot dans une phrase, le modèle calcule à quel point il doit "prêter attention" à tous les autres mots, quelle que soit leur distance. Ce mécanisme d'auto-attention, répété à travers de multiples couches et têtes d'attention parallèles, s'avère extraordinairement puissant. De plus, contrairement aux RNN, les Transformers sont massivement parallélisables, permettant d'exploiter pleinement les accélérateurs modernes.

L'impact des Transformers dépasse rapidement le domaine initial de la traduction automatique. BERT (2018) pré-entraîne un Transformer bidirectionnel sur des corpus massifs, créant des représentations linguistiques d'une richesse inégalée. La série GPT (Generative Pre-trained Transformer) de OpenAI démontre qu'un simple objectif de prédiction du mot suivant, combiné à un modèle suffisamment large et des données suffisantes, produit une compréhension émergente du langage. En 2020, les Vision Transformers (ViT) prouvent que l'architecture Transformer peut aussi dominer la vision par ordinateur, unifiant ainsi les deux domaines sous un même paradigme architectural.

\subsection{Modèles de fondation et ère actuelle}

L'année 2020 marque l'avènement des "modèles de fondation" avec GPT-3, un Transformer comportant 175 milliards de paramètres. Cette échelle, impensable quelques années auparavant, révèle des capacités émergentes stupéfiantes : GPT-3 peut traduire, résumer, programmer, raisonner, et converser avec un niveau de cohérence remarquable, le tout sans entraînement spécifique sur ces tâches. Le paradigme "pre-train + fine-tune" devient dominant : on pré-entraîne un modèle massif sur des données générales, puis on l'adapte à des tâches spécifiques avec un minimum de données supplémentaires.

Cette tendance s'étend rapidement au-delà du texte. DALL-E et DALL-E 2 génèrent des images à partir de descriptions textuelles. Stable Diffusion démocratise la génération d'images en proposant un modèle open-source performant. Des modèles multimodaux comme CLIP apprennent des représentations jointes image-texte, ouvrant la voie à des applications transversales. En 2022-2023, ChatGPT et GPT-4 démontrent que ces modèles peuvent assister efficacement les humains dans une multitude de tâches créatives et analytiques.

Nous sommes aujourd'hui à l'aube d'une nouvelle ère où les frontières entre domaines s'estompent. Les modèles de fondation multimodaux visent à intégrer texte, image, son et vidéo dans une représentation unifiée. L'apprentissage par renforcement avec feedback humain (RLHF) permet d'aligner ces systèmes avec nos valeurs et intentions. Les questions ne sont plus purement techniques mais touchent à l'éthique, la gouvernance et l'impact sociétal de ces technologies transformatrices.

\section{Architectures classiques}

\subsection{Perceptrons multicouches (MLP)}

\subsubsection{Architecture}

Un MLP est une séquence de couches fully-connected :
\[
\mathbf{h}^{(l+1)} = \sigma\left(\mathbf{W}^{(l)}\mathbf{h}^{(l)} + \mathbf{b}^{(l)}\right)
\]

où $\mathbf{W}^{(l)}$ sont les poids, $\mathbf{b}^{(l)}$ les biais, $\sigma$ la fonction d'activation.

\subsubsection{Fonctions d'activation}

\textbf{Sigmoïde} : $\sigma(x) = \frac{1}{1 + e^{-x}}$
\begin{itemize}
    \item Sort dans $[0, 1]$, interprétable comme probabilité
    \item Problème : saturation $\rightarrow$ vanishing gradient
\end{itemize}

\textbf{Tanh} : $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$
\begin{itemize}
    \item Sort dans $[-1, 1]$, centré en 0
    \item Moins de saturation que sigmoïde mais persiste
\end{itemize}

\textbf{ReLU} : $\text{ReLU}(x) = \max(0, x)$
\begin{itemize}
    \item Résout vanishing gradient (gradient = 0 ou 1)
    \item Sparse activation
    \item Problème : dying ReLU (neurones morts)
\end{itemize}

\textbf{Variantes} : Leaky ReLU, PReLU, ELU, GELU, Swish/SiLU

\subsubsection{Rétropropagation}

L'algorithme de rétropropagation calcule efficacement les gradients $\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(l)}}$ via la règle de dérivation en chaîne.

Pour une couche $l$ :
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(l)}} = \frac{\partial \mathcal{L}}{\partial \mathbf{h}^{(l+1)}} \frac{\partial \mathbf{h}^{(l+1)}}{\partial \mathbf{W}^{(l)}} = \delta^{(l+1)} (\mathbf{h}^{(l)})^T
\]

où $\delta^{(l+1)}$ est l'erreur rétropropagée.

\subsection{Réseaux de neurones convolutifs (CNN)}

\subsubsection{Motivation}

Les images possèdent trois propriétés exploitables :
\begin{enumerate}
    \item \textbf{Localité spatiale} : Pixels voisins sont corrélés
    \item \textbf{Stationnarité} : Même features visuelles à différentes positions
    \item \textbf{Hiérarchie} : Features simples (edges) → complexes (objets)
\end{enumerate}

Les CNN exploitent ces propriétés via trois mécanismes : convolution, pooling, architecture hiérarchique.

\subsubsection{Opération de convolution}

Une convolution 2D applique un filtre $\mathbf{K}$ de taille $k \times k$ :
\[
(\mathbf{I} * \mathbf{K})_{i,j} = \sum_{m=0}^{k-1} \sum_{n=0}^{k-1} \mathbf{I}_{i+m, j+n} \cdot \mathbf{K}_{m,n}
\]

\textbf{Propriétés} :
\begin{itemize}
    \item Partage de poids : même filtre appliqué partout
    \item Champ réceptif local : connexions limitées
    \item Équivariance par translation : $f(\text{shift}(x)) = \text{shift}(f(x))$
\end{itemize}

\subsubsection{Pooling}

Le pooling réduit la dimensionnalité :
\begin{itemize}
    \item \textbf{Max pooling} : $p_{i,j} = \max_{(m,n) \in \text{window}} x_{i+m, j+n}$
    \item \textbf{Average pooling} : $p_{i,j} = \frac{1}{k^2}\sum_{(m,n)} x_{i+m, j+n}$
\end{itemize}

Avantages : invariance locale par translation, réduction dimensionnalité, augmentation champ réceptif.

\subsubsection{Architectures CNN emblématiques}

\textbf{LeNet-5 (1998)} :
\begin{verbatim}
Input (32×32) → Conv(6) → Pool → Conv(16) → Pool → FC(120) → FC(84) → Output(10)
\end{verbatim}

\textbf{AlexNet (2012)} :
8 couches, 60M paramètres, ReLU, Dropout, Data augmentation

\textbf{VGG (2014)} :
Empile petits filtres 3×3 (plus efficace que grands filtres)

\textbf{ResNet (2015)} :
Skip connections : $\mathbf{h}^{(l+2)} = \mathbf{h}^{(l)} + F(\mathbf{h}^{(l)})$

\subsubsection{Extension 3D}

Les CNN 3D remplacent convolutions 2D par 3D :
\[
(\mathbf{I} * \mathbf{K})_{i,j,k} = \sum_{l,m,n} \mathbf{I}_{i+l, j+m, k+n} \cdot \mathbf{K}_{l,m,n}
\]

\textbf{Coût computationnel} : Pour filtre $k \times k \times k$ :
\begin{itemize}
    \item 2D : $\mathcal{O}(k^2 HW)$ opérations
    \item 3D : $\mathcal{O}(k^3 HWD)$ opérations
\end{itemize}

Explosion cubique explique les contraintes mémoire des CNN 3D.

\subsection{Réseaux de neurones récurrents (RNN, LSTM, GRU)}

\subsubsection{RNN basiques}

Pour une séquence $(x_1, \ldots, x_T)$, un RNN maintient un état caché $h_t$ :
\[
h_t = \sigma(W_{hh} h_{t-1} + W_{xh} x_t + b_h)
\]

\textbf{Problème} : Vanishing/exploding gradients sur longues séquences.

\subsubsection{LSTM (Long Short-Term Memory)}

Introduit des gates (oubli, entrée, sortie) pour contrôler flux d'information :
\begin{align*}
f_t &= \sigma(W_f [h_{t-1}, x_t] + b_f) \quad \text{(forget gate)} \\
i_t &= \sigma(W_i [h_{t-1}, x_t] + b_i) \quad \text{(input gate)} \\
\tilde{C}_t &= \tanh(W_C [h_{t-1}, x_t] + b_C) \quad \text{(candidate)} \\
C_t &= f_t \odot C_{t-1} + i_t \odot \tilde{C}_t \quad \text{(cell state)} \\
o_t &= \sigma(W_o [h_{t-1}, x_t] + b_o) \quad \text{(output gate)} \\
h_t &= o_t \odot \tanh(C_t)
\end{align*}

\subsection{Transformers et mécanisme d'attention}

\subsubsection{Self-attention}

L'attention calcule une combinaison pondérée de valeurs :
\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
\]

où $Q$ (queries), $K$ (keys), $V$ (values) sont des projections linéaires de l'input.

\subsubsection{Multi-head attention}

Plusieurs têtes d'attention en parallèle capturent différents types de relations :
\[
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W^O
\]

\subsubsection{Architecture Transformer}

Un bloc Transformer combine :
\begin{enumerate}
    \item Multi-head self-attention
    \item Layer normalization + skip connection
    \item Feed-forward MLP
    \item Layer normalization + skip connection
\end{enumerate}

\textbf{Avantages} : Parallélisable, capture dépendances longue distance, scalable.

\textbf{Limitations} : Complexité quadratique $\mathcal{O}(n^2)$ en longueur de séquence.

\section{Techniques d'optimisation}

\subsection{Descente de gradient stochastique (SGD)}

\subsubsection{Gradient descent classique}

Pour minimiser $\mathcal{L}(\theta)$, on itère :
\[
\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}(\theta_t)
\]

où $\eta$ est le learning rate.

\textbf{Limitations} : Lent sur grandes données (calcul gradient sur dataset entier).

\subsubsection{Mini-batch SGD}

À chaque itération, utiliser un mini-batch $\mathcal{B}$ :
\[
\theta_{t+1} = \theta_t - \eta \nabla_\theta \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathcal{L}_i(\theta_t)
\]

\textbf{Avantages} : Rapide, stochastique aide à échapper aux minima locaux, parallélisable sur GPU.

\subsection{Momentum et variantes}

\subsubsection{SGD avec momentum}

Accumule un vecteur de vélocité :
\begin{align*}
v_{t+1} &= \beta v_t + \nabla_\theta \mathcal{L}(\theta_t) \\
\theta_{t+1} &= \theta_t - \eta v_{t+1}
\end{align*}

Typiquement $\beta = 0.9$. Accélère dans directions constantes, amortit oscillations.

\subsubsection{Nesterov Accelerated Gradient (NAG)}

"Regarde avant de sauter" :
\[
v_{t+1} = \beta v_t + \nabla_\theta \mathcal{L}(\theta_t - \eta \beta v_t)
\]

\subsection{Optimiseurs adaptatifs}

\subsubsection{Adam (Adaptive Moment Estimation)}

Combine momentum et adaptation du learning rate par paramètre :
\begin{align*}
m_t &= \beta_1 m_{t-1} + (1-\beta_1) g_t \quad \text{(first moment)} \\
v_t &= \beta_2 v_{t-1} + (1-\beta_2) g_t^2 \quad \text{(second moment)} \\
\hat{m}_t &= \frac{m_t}{1 - \beta_1^t} \quad \text{(bias correction)} \\
\hat{v}_t &= \frac{v_t}{1 - \beta_2^t} \\
\theta_{t+1} &= \theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
\end{align*}

Hyperparamètres typiques : $\beta_1 = 0.9$, $\beta_2 = 0.999$, $\epsilon = 10^{-8}$.

\subsubsection{AdamW}

Variante corrigeant le weight decay :
\[
\theta_{t+1} = \theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} - \eta \lambda \theta_t
\]

Préférable à Adam pour deep learning moderne.

\subsection{Learning rate scheduling}

\subsubsection{Step decay}

Réduire LR par facteur fixe tous les $N$ epochs :
\[
\eta_t = \eta_0 \cdot \gamma^{\lfloor t/N \rfloor}
\]

\subsubsection{Cosine annealing}

Variation douce :
\[
\eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})\left(1 + \cos\left(\frac{t}{T}\pi\right)\right)
\]

\subsubsection{ReduceLROnPlateau}

Adaptatif : réduit LR si métrique de validation stagne (notre choix pour GNN).

\subsection{Initialisation des poids}

\subsubsection{Xavier/Glorot initialization}

Pour activation linéaire/tanh :
\[
W \sim \mathcal{U}\left(-\sqrt{\frac{6}{n_{\text{in}} + n_{\text{out}}}}, \sqrt{\frac{6}{n_{\text{in}} + n_{\text{out}}}}\right)
\]

\subsubsection{He initialization}

Pour ReLU :
\[
W \sim \mathcal{N}\left(0, \frac{2}{n_{\text{in}}}\right)
\]

Compense le fait que ReLU annule 50\% des activations.

\subsection{Batch normalization et variantes}

\subsubsection{Batch Normalization}

Normalise activations par batch~\cite{Ioffe2015} :
\[
\hat{x} = \frac{x - \mu_\mathcal{B}}{\sqrt{\sigma_\mathcal{B}^2 + \epsilon}}
\]

puis applique transformation affine apprise : $y = \gamma \hat{x} + \beta$.

\textbf{Avantages} : Stabilise entraînement, permet learning rates plus élevés, régularise.

\subsubsection{Layer Normalization}

Normalise par features plutôt que par batch~\cite{Ba2016}. Préféré pour GNN (batch size petit, graphes de tailles variables).

\subsubsection{Graph Normalization}

Adaptations spécifiques pour graphes : GraphNorm, MeanSubtractionNorm.

\section{Régularisation}

Le sur-apprentissage (overfitting) survient quand le modèle mémorise les données d'entraînement sans généraliser. Plusieurs techniques combattent ce phénomène.

\subsection{Dropout}

\subsubsection{Principe}

Pendant l'entraînement, chaque neurone est désactivé avec probabilité $p$ (typiquement 0.5). À l'inférence, tous les neurones sont actifs mais leurs sorties sont multipliées par $(1-p)$.

\textbf{Formellement}, pour une couche :
\[
\mathbf{h}^{(l+1)} = \sigma\left(\mathbf{W}^{(l)} (\mathbf{h}^{(l)} \odot \mathbf{m}) + \mathbf{b}^{(l)}\right)
\]

où $\mathbf{m} \in \{0, 1\}^{d}$ est un masque aléatoire avec $P(m_i = 1) = 1-p$.

\textbf{Interprétation} : Dropout entraîne un ensemble de réseaux qui partagent des poids, équivalent à model averaging.

\subsubsection{Variantes}

\textbf{DropConnect} : Désactive connections plutôt que neurones.

\textbf{Spatial Dropout} : Pour CNN, désactive canaux entiers (cohérence spatiale).

\textbf{DropEdge} : Pour GNN, désactive arêtes aléatoirement (notre approche pour robustesse).

\subsection{Weight decay (L2 regularization)}

Ajoute pénalité sur magnitude des poids :
\[
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{data}} + \lambda \sum_l \|\mathbf{W}^{(l)}\|_F^2
\]

Typiquement $\lambda = 10^{-4}$ à $10^{-5}$.

\textbf{Effet} : Encourage poids petits, évite solutions extrêmes, améliore généralisation.

\subsection{Data augmentation}

\subsubsection{Augmentations géométriques}

Pour images~\cite{Shorten2019} :
\begin{itemize}
    \item Rotations aléatoires ($\pm 30°$)
    \item Translations ($\pm 10\%$)
    \item Scaling ($\times 0.8$ à $\times 1.2$)
    \item Flips horizontaux/verticaux
    \item Déformations élastiques
\end{itemize}

\subsubsection{Augmentations photométriques}

\begin{itemize}
    \item Variations de luminosité ($\pm 20\%$)
    \item Variations de contraste
    \item Ajout de bruit gaussien
    \item Flou gaussien
\end{itemize}

\subsubsection{Augmentations pour graphes}

Pour nos graphes cellulaires :
\begin{itemize}
    \item \textbf{Node dropout} : Retirer nœuds aléatoires
    \item \textbf{Edge dropout} : Retirer arêtes aléatoires
    \item \textbf{Feature masking} : Masquer certaines features
    \item \textbf{Subgraph sampling} : Échantillonner sous-graphes
    \item \textbf{Geometric transformations} : Rotations 3D aléatoires (test d'invariance)
\end{itemize}

\subsection{Early stopping}

\subsubsection{Principe}

Monitorer performance sur validation set. Arrêter si pas d'amélioration pendant $N$ epochs (patience).

\textbf{Algorithme} :
\begin{enumerate}
    \item Initialiser $\text{best\_val} = -\infty$, $\text{patience\_counter} = 0$
    \item À chaque epoch : calculer $\text{val\_score}$
    \item Si $\text{val\_score} > \text{best\_val}$ :
        \begin{itemize}
            \item Sauvegarder modèle
            \item $\text{best\_val} = \text{val\_score}$
            \item $\text{patience\_counter} = 0$
        \end{itemize}
    \item Sinon : $\text{patience\_counter} += 1$
    \item Si $\text{patience\_counter} \geq \text{patience}$ : arrêter
\end{enumerate}

\textbf{Notre configuration} : Patience de 20-30 epochs pour GNN.

\subsection{Label smoothing}

Remplace hard labels $(0, 1)$ par soft labels $(0.1, 0.9)$ :
\[
y_{\text{smooth}} = (1 - \epsilon) y_{\text{hard}} + \epsilon / K
\]

où $K$ est le nombre de classes, $\epsilon \approx 0.1$.

\textbf{Effet} : Réduit overconfidence, améliore calibration des probabilités.

\section{Bonnes pratiques d'entraînement}

\subsection{Validation croisée}

\subsubsection{K-fold cross-validation}

Diviser dataset en $K$ folds, entraîner $K$ fois en utilisant chaque fold comme validation. Moyenner les performances.

\textbf{Avantages} : Estimation robuste, utilise toutes les données.

\textbf{Inconvénient} : Coût computationnel $\times K$.

\textbf{Notre pratique} : 5-fold CV pour expériences finales.

\subsubsection{Stratified split}

Pour données déséquilibrées, assurer même distribution de classes dans train/val/test.

\subsubsection{Validation croisée spécialisée pour organoïdes}

\textbf{Patient-level split} :
\begin{itemize}
    \item Éviter le data leakage entre organoïdes du même patient
    \item Stratification par patient plutôt que par organoïde
    \item Garantit la généralisation à nouveaux patients
\end{itemize}

\textbf{Time-based split} :
\begin{itemize}
    \item Train sur les sessions anciennes, test sur les sessions récentes
    \item Valide la stabilité temporelle des modèles
    \item Important pour le déploiement en conditions réelles
\end{itemize}

\textbf{Lab-based split} :
\begin{itemize}
    \item Train sur un laboratoire, test sur un autre
    \item Valide la robustesse aux variations expérimentales
    \item Crucial pour la généralisation multi-sites
\end{itemize}

\subsection{Monitoring et visualisation}

\subsubsection{TensorBoard}

Monitoring en temps réel :
\begin{itemize}
    \item Courbes de loss (train/val)
    \item Métriques (accuracy, F1, AUC)
    \item Distributions de poids et gradients
    \item Histogrammes d'activations
\end{itemize}

\subsubsection{Weights \& Biases (Wandb)}

Plateforme cloud pour tracking d'expériences, comparaisons, partage de résultats.

\subsection{Checkpointing}

Sauvegarder régulièrement :
\begin{itemize}
    \item \textbf{Best model} : Meilleure performance validation
    \item \textbf{Latest model} : Dernier epoch
    \item \textbf{Epoch checkpoints} : Tous les N epochs (pour analyse post-hoc)
\end{itemize}

\textbf{Format} : Dictionnaire PyTorch incluant :
\begin{verbatim}
{
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'scheduler_state_dict': scheduler.state_dict(),
    'train_loss': train_loss,
    'val_loss': val_loss,
    'val_metrics': metrics_dict,
    'config': config,
}
\end{verbatim}

\subsection{Hyperparamètre tuning}

\subsubsection{Grid search}

Test exhaustif de combinaisons. Coûteux mais complet.

\subsubsection{Random search}

Plus efficace que grid search pour espaces haute dimension.

\subsubsection{Bayesian optimization}

Modélise $f(\text{hyperparams}) \rightarrow \text{performance}$ par processus gaussien, propose séquentiellement les hyperparamètres à tester. Outils : Optuna, Hyperopt.

\subsubsection{Notre approche}

\begin{enumerate}
    \item Grid search grossier pour identifier régions prometteuses
    \item Random search fine autour de ces régions
    \item Validation finale avec 5-fold CV sur meilleure config
\end{enumerate}

\chapter{Compléments sur les graphes et GNNs}

Ce chapitre approfondit les aspects avancés de la théorie des graphes et des GNN non couverts dans le Chapitre 3.

\section{Expressivité et limitations théoriques des GNNs}

Cette section développe les aspects théoriques de l'expressivité des Graph Neural Networks, leurs limitations fondamentales, et les solutions proposées.

\subsection{Weisfeiler-Lehman test}

\subsubsection{Algorithme WL}

Le test de Weisfeiler-Lehman (1-WL) est un algorithme itératif testant l'isomorphisme de graphes :

\begin{enumerate}
    \item Initialiser chaque nœud avec un label (degré ou label initial)
    \item Itérer : nouveau label = hash(label ancien, multiset labels voisins)
    \item Comparer les multisets de labels finaux des deux graphes
\end{enumerate}

Si les multisets diffèrent, les graphes sont non-isomorphes. Sinon, inconnu.

\subsubsection{Lien avec les GNNs}

Xu et al.~\cite{Xu2019} ont prouvé que les GNNs utilisant somme+agrégation sont au plus aussi puissants que 1-WL. GIN atteint cette borne. GCN et GAT sont strictement moins expressifs.

\textbf{Implications :}
\begin{itemize}
    \item Certains graphes non-isomorphes sont indistinguables par GNNs standards
    \item Pour la classification d'organoïdes, c'est généralement suffisant (graphes très différents)
    \item Pour tâches nécessitant discrimination fine, des architectures plus puissantes (k-WL, higher-order) peuvent être nécessaires
\end{itemize}

Des analyses théoriques récentes~\cite{Morris2023WL,Grohe2023,Boker2023} ont approfondi notre compréhension de l'expressivité des GNNs, établissant des liens avec la logique et la complexité descriptive. Des extensions au-delà de 1-WL incluent les réseaux cellulaires~\cite{Bodnar2022CW} et simpliciaux~\cite{Bodnar2021Simplicial}, qui augmentent l'expressivité en opérant sur des structures topologiques plus riches que les graphes.

\subsection{Over-smoothing}

\subsubsection{Phénomène}

Avec un nombre de couches $K$ croissant, les représentations de tous les nœuds convergent vers une valeur commune, perdant toute information de structure.

\textbf{Intuition :}
Chaque couche "dilue" l'information en moyennant avec les voisins. Après $K$ couches, tous les nœuds ont accès à une information similaire (agrégation sur $K$-hop voisinage), rendant leurs représentations indistinguables.

\textbf{Analyse théorique :}
Pour GCN, les représentations convergent vers l'espace propre dominant du Laplacien normalisé.

\subsubsection{Conséquences pratiques}

\begin{itemize}
    \item Les GNNs standards sont limités à 2-4 couches en pratique
    \item Les réseaux profonds (> 8 couches) voient leurs performances s'effondrer
    \item Problématique pour graphes de grand diamètre nécessitant propagation longue distance
\end{itemize}

\subsubsection{Solutions proposées}

\begin{itemize}
    \item \textbf{Skip connections} : Connexions résiduelles préservant information initiale
    \item \textbf{Initial residual connections} : $\mathbf{h}_i^{(k+1)} = \mathbf{h}_i^{(k)} + \Delta\mathbf{h}_i^{(k)}$
    \item \textbf{Jumping Knowledge} : Concaténer représentations de toutes les couches
    \item \textbf{PairNorm, DGN} : Normalisation des représentations pour préserver diversité
\end{itemize}

\subsection{Over-squashing}

\subsubsection{Problème de goulets d'étranglement}

L'information doit traverser des goulets d'étranglement topologiques (nœuds de faible degré, coupes minimales) pour se propager. Cela cause une compression/perte d'information~\cite{Alon2021}, phénomène désormais bien compris théoriquement~\cite{DiGiovanni2023,Black2023}.

\textbf{Exemple :}
Dans un graphe en "haltère" (deux clusters denses connectés par un nœud pont), l'information des deux clusters doit passer par le nœud pont, créant un bottleneck.

\subsubsection{Relation avec courbure}

Des travaux récents~\cite{Topping2022,Nguyen2023} lient l'over-squashing à la courbure discrète des graphes (courbure d'Ollivier-Ricci). Les graphes de courbure négative (expansifs) facilitent la propagation ; les graphes de courbure positive (contractifs) l'entravent. Cette perspective géométrique offre des outils quantitatifs pour analyser et atténuer l'over-squashing.

\subsubsection{Atténuation}

\begin{itemize}
    \item Rewiring du graphe pour améliorer la connectivité
    \item Ajout d'arêtes longue-distance
    \item Architectures avec agrégation multi-échelles
\end{itemize}

\section{Types de graphes exotiques}

\subsection{Hypergraphes}

\subsubsection{Définition}

Un hypergraphe $\mathcal{H} = (V, \mathcal{E})$ généralise les graphes en permettant aux hyperarêtes $e \in \mathcal{E}$ de connecter un nombre arbitraire de nœuds : $e \subseteq V$ avec $|e| \geq 2$.

\textbf{Exemple biologique} : Une interaction protéique peut impliquer 3+ protéines simultanément.

\subsubsection{Hypergraph Neural Networks}

Extension des GNN aux hypergraphes via :
\begin{itemize}
    \item Message passing nœuds → hyperarêtes → nœuds
    \item Incidence matrix $\mathbf{H} \in \{0,1\}^{|V| \times |\mathcal{E}|}$
    \item Convolutions spectrales sur hypergraphes
\end{itemize}

\textbf{Application potentielle} : Modéliser interactions cellulaires multi-voies (signalisation paracrine, contacts mécaniques, gap junctions).

\subsection{Graphes dynamiques}

\subsubsection{Graphes temporels}

Un graphe temporel est une séquence $(G_1, G_2, \ldots, G_T)$ où la topologie et/ou les features évoluent.

\textbf{Cas d'usage pour organoïdes} : Tracking de croissance, division cellulaire, réorganisation spatiale.

\subsubsection{Temporal GNN}

Architectures combinant GNN spatial + RNN/LSTM temporel :
\[
\mathbf{h}_i^{(t)} = \text{GNN}\left(\{\mathbf{h}_j^{(t-1)} : j \in \mathcal{N}_i^{(t)}\}\right)
\]

\textbf{Applications} : Prédiction de dynamiques, détection d'événements (division, mort cellulaire).

\subsection{Graphes hétérogènes}

\subsubsection{Définition}

Un graphe hétérogène possède plusieurs types de nœuds $\mathcal{V} = \bigcup_{\tau \in \mathcal{T}_v} V_\tau$ et d'arêtes $\mathcal{E} = \bigcup_{\phi \in \mathcal{T}_e} E_\phi$.

\textbf{Exemple organoïdes} :
\begin{itemize}
    \item Types de nœuds : cellules souches, cellules différenciées, cellules en apoptose
    \item Types d'arêtes : contact direct, interaction paracrine, mechanical stress
\end{itemize}

\subsubsection{Heterogeneous GNN (HAN, RGCN)}

Message passing spécialisé par type :
\[
\mathbf{h}_i^{(l+1)} = \sigma\left(\sum_{\phi \in \mathcal{T}_e} \sum_{j \in \mathcal{N}_i^\phi} \mathbf{W}_\phi^{(l)} \mathbf{h}_j^{(l)}\right)
\]

\section{Geometric Deep Learning : formalisme unifié}

\subsection{Principe fondamental}

Le Geometric Deep Learning~\cite{Bronstein2021} propose un cadre théorique unifié pour le deep learning sur domaines non-euclidiens (graphes, variétés, groupes, ensembles) basé sur les symétries et invariances.

\subsubsection{Théorème fondamental}

Toute architecture de deep learning peut être caractérisée par :
\begin{enumerate}
    \item Le \textbf{domaine} sur lequel elle opère (grille, graphe, variété, etc.)
    \item Les \textbf{symétries} qu'elle respecte (translations, rotations, permutations, etc.)
    \item L'\textbf{échelle} à laquelle elle opère (locale, globale, multi-échelle)
\end{enumerate}

\subsection{Hiérarchie des symétries}

\textbf{Grilles régulières (images)} :
\begin{itemize}
    \item Symétrie : groupe de translation $\mathbb{Z}^d$
    \item Architecture : CNN (convolution = équivariance par translation)
\end{itemize}

\textbf{Ensembles (point clouds)} :
\begin{itemize}
    \item Symétrie : permutations $S_n$
    \item Architecture : DeepSets~\cite{Zaheer2017} (théorème de représentation universelle pour fonctions invariantes aux permutations : $f(\mathcal{X}) = \rho(\sum_i \phi(x_i))$), PointNet~\cite{Qi2017} (application aux nuages de points 3D avec max-pooling)
    \item Limitation : agrégation globale, pas de structure locale
\end{itemize}

\textbf{Graphes} :
\begin{itemize}
    \item Symétrie : permutations des nœuds
    \item Architecture : GNN (message passing invariant)
\end{itemize}

\textbf{Graphes géométriques} :
\begin{itemize}
    \item Symétrie : groupe euclidien $E(d)$ (translations + rotations + réflexions)
    \item Architecture : EGNN, SchNet (équivariance géométrique)
\end{itemize}

\subsection{Blueprint du Geometric Deep Learning}

Recette générale pour concevoir une architecture :
\begin{enumerate}
    \item Identifier le domaine et ses symétries
    \item Construire des opérations linéaires équivariantes
    \item Ajouter des non-linéarités point-wise (préservent équivariance)
    \item Composer en réseau profond
\end{enumerate}

\section{Expressivité théorique : au-delà du WL-test}

\subsection{Rappel : limitations du 1-WL test}

Le 1-WL test (Weisfeiler-Leman) itère :
\begin{align*}
c_i^{(0)} &= \text{label initial} \\
c_i^{(k+1)} &= \text{HASH}\left(c_i^{(k)}, \{\!\!\{c_j^{(k)} : j \in \mathcal{N}(i)\}\!\!\}\right)
\end{align*}

La plupart des GNN standards (GCN, GAT, GraphSAGE) sont au mieux aussi puissants que 1-WL~\cite{Xu2019}.

\subsection{Hiérarchie WL}

\subsubsection{k-WL test}

Le k-WL opère sur k-tuples de nœuds au lieu de nœuds individuels.

\textbf{2-WL} : Considère paires $(i, j)$, peut distinguer plus de graphes.

\textbf{k-WL pour $k \geq 3$} : Encore plus puissant, mais coût combinatoire $\mathcal{O}(n^k)$.

\subsubsection{Higher-order GNNs}

\textbf{k-GNN}~\cite{Morris2019} : Opère sur k-tuples, atteint expressivité k-WL.

\textbf{Coût} : $\mathcal{O}(n^k)$ nœuds dans le graphe augmenté. Prohibitif pour $k \geq 3$.

\subsection{Alternatives à higher-order}

\textbf{Subgraph GNN} : Compte occurrences de motifs (triangles, cycles).

\textbf{Random features} : Ajoute features aléatoires pour briser symétries.

\textbf{Positional encodings} : Encode position relative des nœuds.

\subsection{En pratique : expressivité nécessaire ?}

Pour la plupart des tâches réelles, 1-WL expressivité suffit. Les graphes pathologiques non-distinguables par 1-WL sont rares en pratique.

\textbf{Notre contexte (organoïdes)} : Graphes géométriques avec positions 3D fournissent déjà information suffisante pour distinguer structures. Pas besoin de higher-order.

\section{Méthodes d'interprétabilité pour GNNs}

\subsection{GNNExplainer}

\subsubsection{Principe}

GNNExplainer~\cite{Ying2019} identifie les sous-graphes et features les plus importants pour une prédiction donnée en optimisant :

\[
\max_{G_S, F_S} MI(Y, (G_S, F_S))
\]

où $G_S$ est un sous-graphe, $F_S$ un sous-ensemble de features, et $MI$ l'information mutuelle.

\subsubsection{Algorithme}

\begin{enumerate}
    \item Initialiser masque d'arêtes $\mathbf{M}_E$ et masque de features $\mathbf{M}_F$
    \item Optimiser via gradient ascent :
    \[
    \mathbf{M}_E, \mathbf{M}_F = \arg\max \log p_\theta(Y \mid G_S, F_S)
    \]
    \item Sélectionner arêtes/features avec probabilité > seuil
\end{enumerate}

\subsubsection{Application aux organoïdes}

\textbf{Interprétation biologique} :
\begin{itemize}
    \item Arêtes importantes : contacts cellulaires critiques
    \item Features importantes : volume, position, intensité
    \item Sous-graphes : régions fonctionnelles (cryptes, lumens)
\end{itemize}

\subsection{Attention weights et visualisation}

\subsubsection{Attention dans GAT}

Pour GAT, les poids d'attention $\alpha_{ij}$ indiquent l'importance relative des voisins :

\[
\alpha_{ij} = \frac{\exp(\text{LeakyReLU}(\mathbf{a}^T [\mathbf{W}\mathbf{h}_i \| \mathbf{W}\mathbf{h}_j]))}{\sum_{k \in \mathcal{N}(i)} \exp(\text{LeakyReLU}(\mathbf{a}^T [\mathbf{W}\mathbf{h}_i \| \mathbf{W}\mathbf{h}_k]))}
\]

\subsubsection{Visualisation 3D}

\textbf{Heatmaps d'attention} : Couleur des nœuds proportionnelle à $\sum_j \alpha_{ij}$

\textbf{Arêtes pondérées} : Épaisseur proportionnelle à $\alpha_{ij}$

\textbf{Animation temporelle} : Évolution des poids au fil des couches

\subsection{GradCAM pour graphes}

\subsubsection{Extension de GradCAM}

Adaptation de GradCAM~\cite{Selvaraju2017} aux GNNs :

\[
L_{GNN-CAM}^c = \text{ReLU}\left(\sum_i \alpha_i^c \cdot \mathbf{h}_i\right)
\]

où $\alpha_i^c = \frac{\partial y^c}{\partial \mathbf{h}_i}$ est le gradient de la classe $c$ par rapport aux features du nœud $i$.

\subsubsection{Implémentation}

\begin{verbatim}
def gnn_gradcam(model, data, class_idx):
    model.eval()
    data.requires_grad_(True)
    
    output = model(data)
    score = output[0, class_idx]
    
    # Backward pass
    score.backward()
    
    # Gradients w.r.t. node features
    gradients = data.x.grad
    
    # Global average pooling
    alpha = gradients.mean(dim=1)
    
    # Weighted combination
    cam = torch.relu(alpha.unsqueeze(1) * data.x).sum(dim=1)
    
    return cam
\end{verbatim}

\subsection{Perturbation analysis}

\subsubsection{Node perturbation}

Mesurer l'impact de retirer un nœud $i$ :

\[
\text{Impact}_i = |f(G) - f(G \setminus \{i\})|
\]

où $f$ est la fonction de prédiction du GNN.

\subsubsection{Edge perturbation}

Mesurer l'impact de retirer une arête $(i,j)$ :

\[
\text{Impact}_{ij} = |f(G) - f(G \setminus \{(i,j)\})|
\]

\subsubsection{Feature ablation}

Mesurer l'impact de masquer une feature $d$ :

\[
\text{Impact}_d = |f(G, \mathbf{X}) - f(G, \mathbf{X}_{-d})|
\]

où $\mathbf{X}_{-d}$ masque la dimension $d$.

\section{Message passing généralisé et extensions}

\subsection{Message passing avec edge updates}

Au-delà du MPNN standard, mettre à jour aussi les arêtes :
\begin{align*}
\mathbf{m}_{ij} &= \phi_e\left(\mathbf{h}_i, \mathbf{h}_j, \mathbf{e}_{ij}\right) \\
\mathbf{e}_{ij}' &= \mathbf{e}_{ij} + \mathbf{m}_{ij} \\
\mathbf{h}_i' &= \phi_v\left(\mathbf{h}_i, \sum_{j \in \mathcal{N}(i)} \mathbf{m}_{ij}\right)
\end{align*}

\textbf{Utilité} : Raffiner représentation des relations au fil des couches.

\subsection{Attention multi-échelles}

Combiner attention locale (voisins directs) et globale (tous les nœuds) :
\[
\mathbf{h}_i' = \alpha_{\text{local}} \sum_{j \in \mathcal{N}(i)} a_{ij} \mathbf{h}_j + \alpha_{\text{global}} \sum_{j \in V} a_{ij}' \mathbf{h}_j
\]

\textbf{Graph Transformers} : Attention sur tous les nœuds (global), complexité $\mathcal{O}(n^2)$.

\subsection{Graph pooling hierarchical}

\subsubsection{Motivation}

Réduction progressive de la taille du graphe pour capture multi-échelle.

\subsubsection{Méthodes}

\textbf{Top-K pooling} : Garde top-K nœuds selon un score appris.

\textbf{DiffPool}~\cite{Ying2018} : Assigne nœuds à clusters de manière différentiable :
\[
\mathbf{S} = \text{softmax}(\text{GNN}(\mathbf{X}, \mathbf{A}))
\]

Nouveau graphe coarse : $\mathbf{X}' = \mathbf{S}^T \mathbf{X}$, $\mathbf{A}' = \mathbf{S}^T \mathbf{A} \mathbf{S}$.

\textbf{SAGPool, EdgePool, etc.} : Variantes avec différentes stratégies.

\subsubsection{Application organoïdes}

Hierarchical pooling pourrait capturer :
\begin{itemize}
    \item Niveau 1 : Cellules individuelles
    \item Niveau 2 : Micro-domaines (clusters de cellules similaires)
    \item Niveau 3 : Régions fonctionnelles (cryptes, lumens)
    \item Niveau 4 : Organoïde entier
\end{itemize}

\textbf{Perspective future} : À explorer pour classification fine.

\chapter{Processus ponctuels sur la sphère}

Ce chapitre se concentre spécifiquement sur les processus ponctuels définis sur la sphère $\mathbb{S}^2$, adaptés à la modélisation des organoïdes sphériques.

\section{Géométrie de la sphère}

\subsection{Coordonnées et métrique}

Pour une sphère de rayon $R$, les coordonnées sphériques $(r, \theta, \phi)$ avec $r = R$ constant donnent :

\[
\begin{cases}
x = R\sin\phi\cos\theta \\
y = R\sin\phi\sin\theta \\
z = R\cos\phi
\end{cases}
\]

où $\theta \in [0, 2\pi]$ (azimuth) et $\phi \in [0, \pi]$ (colatitude).

\subsection{Distance géodésique}

La distance géodésique entre deux points $\mathbf{p}_1, \mathbf{p}_2$ sur la sphère est :

\[
d_{\text{geo}}(\mathbf{p}_1, \mathbf{p}_2) = R \arccos\left(\frac{\mathbf{p}_1 \cdot \mathbf{p}_2}{R^2}\right)
\]

\textbf{Propriété} : $d_{\text{geo}} \in [0, \pi R]$ (diamètre de la sphère).

\subsection{Élément de surface}

L'élément de surface sur la sphère est :
\[
dS = R^2 \sin\phi \, d\phi \, d\theta
\]

L'aire totale de la sphère est $4\pi R^2$.

\section{Processus de Poisson sur la sphère}

\subsection{Processus de Poisson homogène}

Un processus de Poisson homogène sur $\mathbb{S}^2$ d'intensité $\lambda$ satisfait :

\textbf{P1. Nombre de points} : $N(\mathbb{S}^2) \sim \text{Poisson}(\lambda \cdot 4\pi R^2)$

\textbf{P2. Uniformité} : Les points sont uniformément distribués sur la surface sphérique.

\subsection{Simulation}

\subsubsection{Méthode directe}

\begin{enumerate}
    \item Tirer $N \sim \text{Poisson}(\lambda \cdot 4\pi R^2)$
    \item Pour chaque point :
        \begin{itemize}
            \item $\theta \sim \mathcal{U}([0, 2\pi])$
            \item $\cos\phi \sim \mathcal{U}([-1, 1])$ (uniforme sur sphère)
            \item Convertir en coordonnées cartésiennes
        \end{itemize}
\end{enumerate}

\subsubsection{Rejection sampling}

Alternative pour intensités variables :
\begin{enumerate}
    \item Générer points uniformément dans le cube $[-R, R]^3$
    \item Projeter sur la sphère : $\mathbf{p} \leftarrow R \frac{\mathbf{p}}{\|\mathbf{p}\|}$
    \item Accepter avec probabilité $\lambda(\mathbf{p})/\lambda_{\max}$
\end{enumerate}

\section{Processus inhomogènes sur la sphère}

\subsection{Fonctions d'intensité}

\subsubsection{Gradient latitudinal}

\[
\lambda(\theta, \phi) = \lambda_0 + \lambda_1 \cos\phi
\]

Intensité maximale aux pôles ($\phi = 0, \pi$), minimale à l'équateur ($\phi = \pi/2$).

\subsubsection{Gradient longitudinal}

\[
\lambda(\theta, \phi) = \lambda_0 + \lambda_1 \cos\theta
\]

Modélise une polarisation directionnelle.

\subsubsection{Clustering polaire}

\[
\lambda(\theta, \phi) = \lambda_0 + \lambda_1 \exp\left(-\frac{\phi^2}{2\sigma^2}\right)
\]

Concentration autour du pôle nord ($\phi = 0$).

\section{Statistiques de second ordre sur la sphère}

\subsection{Fonction K de Ripley sphérique}

Pour un processus d'intensité $\lambda$ sur la sphère :

\[
K(r) = \frac{1}{\lambda} \mathbb{E}\left[\sum_{i \neq j} \mathbb{1}(d_{\text{geo}}(x_i, x_j) < r)\right]
\]

\textbf{Estimateur empirique} :
\[
\hat{K}(r) = \frac{4\pi R^2}{n(n-1)} \sum_{i=1}^n \sum_{j \neq i} \mathbb{1}(d_{\text{geo}}(x_i, x_j) < r) w_{ij}
\]

où $w_{ij}$ corrige les effets de bord sphériques.

\textbf{Valeurs théoriques} : Pour un processus de Poisson homogène sur la sphère :
\[
K_{\text{Poisson}}(r) = 4\pi R^2 (1 - \cos(r/R))
\]

\subsection{Fonction F sphérique (nearest neighbor)}

Distribution de la distance au plus proche voisin sur la sphère :

\[
F(r) = P(\text{distance géodésique au NN} \leq r)
\]

\textbf{Estimateur empirique} :
\[
\hat{F}(r) = \frac{1}{n} \sum_{i=1}^n \mathbb{1}(d_{\text{geo}}(x_i, \text{NN}(x_i)) \leq r)
\]

\textbf{Valeur théorique pour Poisson} :
\[
F_{\text{Poisson}}(r) = 1 - \exp\left(-\lambda \cdot 2\pi R^2 (1 - \cos(r/R))\right)
\]

\subsection{Fonction G sphérique (event-to-event)}

Distribution de distance entre points sur la sphère :

\[
G(r) = P(\text{distance géodésique d'un point typique à son NN} \leq r)
\]

\textbf{Estimateur empirique} :
\[
\hat{G}(r) = \frac{1}{n} \sum_{i=1}^n \mathbb{1}(d_{\text{geo}}(x_i, \text{NN}(x_i)) \leq r)
\]

\textbf{Propriété} : Pour un processus de Poisson, $G = F$ (propriété de Slivnyak).

\subsection{Interprétation des fonctions}

\textbf{Fonction K} :
\begin{itemize}
    \item $K(r) > K_{\text{Poisson}}(r)$ : Clustering à distance $r$
    \item $K(r) < K_{\text{Poisson}}(r)$ : Répulsion à distance $r$
\end{itemize}

\textbf{Fonction F} :
\begin{itemize}
    \item $F(r) > F_{\text{Poisson}}(r)$ : Plus de voisins proches (clustering)
    \item $F(r) < F_{\text{Poisson}}(r)$ : Moins de voisins proches (répulsion)
\end{itemize}

\textbf{Fonction G} : Même interprétation que F pour les processus de Poisson.

\section{Processus de Matérn sur la sphère}

\subsection{Construction}

Le processus de Matérn sphérique génère des clusters via :

\begin{enumerate}
    \item Générer centres de clusters via Poisson d'intensité $\kappa$ sur la sphère
    \item Autour de chaque centre, générer points dans un cap sphérique de rayon $r$
    \item Retirer les centres, garder seulement les points générés
\end{enumerate}

\subsection{Cap sphérique}

Un cap de rayon $r$ autour d'un point $\mathbf{c}$ contient tous les points à distance géodésique $< r$ de $\mathbf{c}$.

L'aire d'un cap de rayon $r$ est :
\[
A_{\text{cap}}(r) = 2\pi R^2 (1 - \cos(r/R))
\]

\subsection{Paramètres}

\begin{itemize}
    \item $\kappa$ : intensité des centres (nombre de clusters)
    \item $\mu$ : nombre moyen de points par cluster
    \item $r$ : rayon de clustering (en radians ou distance géodésique)
\end{itemize}

\section{Processus de Strauss sur la sphère}

\subsection{Énergie d'interaction}

Le processus de Strauss sphérique définit :

\[
U(\mathbf{x}) = -\alpha n(\mathbf{x}) - \beta s_R(\mathbf{x})
\]

où $s_R(\mathbf{x})$ compte les paires à distance géodésique $< R$.

\subsection{Simulation MCMC}

\begin{enumerate}
    \item Initialiser avec Poisson uniforme
    \item Proposer déplacements sur la sphère (rotations aléatoires)
    \item Accepter/rejeter selon ratio de Metropolis-Hastings
    \item Itérer jusqu'à convergence
\end{enumerate}

\subsection{Déplacements sur la sphère}

Pour déplacer un point $\mathbf{p}$ :
\begin{enumerate}
    \item Choisir un axe de rotation aléatoire
    \item Appliquer une rotation d'angle $\delta \sim \mathcal{N}(0, \sigma^2)$
    \item Projeter le résultat sur la sphère
\end{enumerate}

\section{Applications aux organoïdes}

\subsection{Modélisation des gradients}

\textbf{Gradient radial} : Intensité décroissante du centre vers la périphérie
\[
\lambda(\phi) = \lambda_0 \exp(-\alpha \phi)
\]

\textbf{Gradient polaire} : Concentration aux pôles (division asymétrique)
\[
\lambda(\phi) = \lambda_0 + \lambda_1 \cos^2\phi
\]

\subsection{Validation statistique}

Pour valider nos processus synthétiques :

\begin{enumerate}
    \item Calculer $\hat{K}(r)$, $\hat{F}(r)$, $\hat{G}(r)$ empiriques
    \item Comparer aux valeurs théoriques attendues
    \item Construire enveloppes Monte Carlo (99 simulations)
    \item Vérifier que les données synthétiques respectent les enveloppes
\end{enumerate}

\subsection{Exemples de résultats}

\textbf{Poisson sphérique} : 
\begin{itemize}
    \item $K(r) = K_{\text{Poisson}}(r)$ pour tous $r$
    \item $F(r) = F_{\text{Poisson}}(r)$ et $G(r) = G_{\text{Poisson}}(r)$
\end{itemize}

\textbf{Matérn sphérique} : 
\begin{itemize}
    \item $K(r) > K_{\text{Poisson}}(r)$ pour $r < r_{\text{cluster}}$
    \item $F(r) > F_{\text{Poisson}}(r)$ (plus de voisins proches)
\end{itemize}

\textbf{Strauss sphérique} : 
\begin{itemize}
    \item $K(r) < K_{\text{Poisson}}(r)$ pour $r < R_{\text{interaction}}$
    \item $F(r) < F_{\text{Poisson}}(r)$ (moins de voisins proches)
\end{itemize}

\chapter{Détails d'implémentation}

Ce chapitre décrit l'implémentation logicielle permettant la reproduction de nos résultats.

\section{Environnement technique}

\subsection{Stack technologique}

\textbf{Langages et frameworks :}
\begin{itemize}
    \item Python 3.9 : Langage principal
    \item PyTorch 2.0 : Framework de deep learning
    \item PyTorch Geometric 2.3 : Bibliothèque spécialisée GNN
\end{itemize}

\textbf{Traitement d'images :}
\begin{itemize}
    \item Cellpose 2.2 : Segmentation cellulaire 3D
    \item Faster Cellpose : Optimisation développée (5× accélération, F1=0.95)
    \item scikit-image 0.20 : Opérations morphologiques
    \item OpenCV 4.7 : Opérations bas-niveau
\end{itemize}

\textbf{Calcul scientifique :}
\begin{itemize}
    \item NumPy 1.24, SciPy 1.10 : Calculs numériques
    \item Pandas 2.0 : Manipulation de données
    \item scikit-learn 1.2 : Baselines et métriques
\end{itemize}

\subsection{Visualisation et monitoring}

\textbf{Visualisation :} Matplotlib, Seaborn, PyVista (3D), Plotly (interactif)

\textbf{Monitoring :} TensorBoard, Weights \& Biases

\textbf{Infrastructure :} Docker, Git/GitHub, CI/CD

\section{Architecture logicielle}

\subsection{Organisation du code}

Codebase modulaire (~5,000 lignes) organisée en modules :
\begin{itemize}
    \item \texttt{data/} : Gestion des datasets PyG
    \item \texttt{models/} : Architectures GNN (GCN, GAT, EGNN, etc.)
    \item \texttt{utils/} : Segmentation, construction de graphes, features
    \item \texttt{synthetic/} : Génération de données synthétiques
    \item \texttt{scripts/} : Scripts d'entraînement et évaluation
    \item \texttt{visualization/} : Visualisation et interprétabilité
\end{itemize}

\subsection{Optimisations}

\textbf{Hardware :} RTX 3090 (développement), A100/V100 (entraînement final)

\textbf{Optimisations mémoire :} Mixed precision (FP16), gradient accumulation, checkpointing

\textbf{Performance :} 4-6h d'entraînement par modèle complet

