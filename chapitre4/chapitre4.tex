% !TEX root = ../sommaire.tex

\chapter{Méthodologie et pipeline de traitement}

\section{Architecture générale du pipeline}

\subsection{Vue d'ensemble : de l'image brute au graphe}

Notre pipeline transforme des images 3D d'organoïdes en graphes géométriques annotés, prêts pour l'analyse par GNN. Les étapes principales sont :
\begin{enumerate}
    \item Acquisition et prétraitement des images
    \item Segmentation cellulaire automatisée
    \item Extraction et séparation des organoïdes
    \item Construction de graphes géométriques
    \item Classification par Graph Neural Networks
\end{enumerate}

\subsection{Choix de conception et compromis}

Chaque étape du pipeline résulte de choix motivés par des contraintes scientifiques et techniques :
\begin{itemize}
    \item \textbf{Segmentation} : Compromis précision/temps de calcul
    \item \textbf{Features} : Sélection des propriétés cellulaires les plus informatives
    \item \textbf{Connectivité} : Équilibre entre densité du graphe et signification biologique
    \item \textbf{Architecture GNN} : Expressivité vs complexité computationnelle
\end{itemize}

\subsection{Justifications scientifiques et techniques}

Le choix d'une représentation par graphes géométriques est motivé par :
\begin{itemize}
    \item Réduction drastique de la dimensionnalité (Go $\rightarrow$ Mo)
    \item Modélisation explicite des relations cellulaires
    \item Invariance naturelle aux transformations géométriques
    \item Interprétabilité : identification de cellules/interactions clés
\end{itemize}

\section{Acquisition et prétraitement des images}

\subsection{Protocoles expérimentaux et marqueurs fluorescents}

Les organoïdes sont imagés en microscopie confocale ou light-sheet après marquage fluorescent. Les marqueurs typiques incluent :
\begin{itemize}
    \item DAPI ou Hoechst pour les noyaux cellulaires
    \item Marqueurs spécifiques de lignages (Sox2, Oct4, etc.)
    \item Marqueurs de prolifération (Ki67) ou d'apoptose (caspase-3)
\end{itemize}

\subsection{Normalisation d'intensité et débruitage}

Les variations d'intensité liées à l'acquisition sont corrigées par :
\begin{itemize}
    \item Normalisation par percentiles pour robustesse aux outliers
    \item Filtrage médian 3D pour réduction du bruit
    \item Correction d'illumination par estimation du fond
\end{itemize}

\subsection{Correction d'artefacts d'acquisition}

Les artefacts communs (perte de signal en profondeur, aberrations chromatiques) sont traités par des méthodes adaptées selon la modalité d'imagerie utilisée.

\subsection{Gestion de la variabilité inter-expérimentale}

Pour assurer la robustesse, nous normalisons les protocoles et appliquons des stratégies d'augmentation de données simulant différentes conditions expérimentales.

\section{Segmentation cellulaire automatisée}

\subsection{Revue des méthodes}

Nous avons évalué plusieurs approches de segmentation :
\begin{itemize}
    \item \textbf{Approches géométriques} : Détection d'ellipsoïdes, rapide mais limitée
    \item \textbf{Watershed} : Classique, tendance à sur-segmenter
    \item \textbf{Stardist} : Prédiction de distances radiales, bon pour noyaux convexes
    \item \textbf{Cellpose} : Deep learning avec flux de gradients, état de l'art
\end{itemize}

\subsection{Comparaison quantitative}

Les méthodes sont évaluées via :
\begin{itemize}
    \item Coefficient de Dice
    \item Intersection over Union (IoU)
    \item Précision et rappel au niveau objet
    \item Average Precision (AP)
\end{itemize}

\subsection{Choix et paramétrage de l'outil optimal}

Basé sur notre évaluation comparative, Cellpose offre le meilleur compromis précision/généralisation. Nous détaillons le paramétrage optimal (diamètre cellulaire, seuils, etc.).

\subsection{Validation qualitative avec experts biologistes}

Une validation qualitative par inspection visuelle avec des biologistes experts confirme la pertinence des segmentations obtenues.

\section{Extraction et séparation des organoïdes}

\subsection{Conversion en nuages de points 3D}

Les masques de segmentation sont convertis en nuages de points où chaque cellule est représentée par les coordonnées de son centroïde et ses propriétés morphologiques.

\subsection{Clustering spatial}

Les cellules appartenant à des organoïdes distincts sont séparées par clustering spatial (DBSCAN) basé sur la proximité géométrique :
\begin{itemize}
    \item Paramètre $\epsilon$ : distance maximale entre cellules d'un même cluster
    \item Paramètre $\text{minPts}$ : nombre minimal de cellules pour former un organoïde
\end{itemize}

\subsection{Filtrage des artefacts et débris cellulaires}

Les clusters de petite taille (< 10 cellules) sont considérés comme débris et éliminés. Des critères de compacité spatiale permettent de filtrer les faux positifs.

\subsection{Identification et isolation des organoïdes individuels}

Chaque cluster valide est extrait comme un organoïde indépendant, formant l'unité d'analyse pour la suite du pipeline.

\section{Construction de graphes géométriques}

\subsection{Définition des nœuds : propriétés cellulaires}

Chaque cellule devient un nœud du graphe, caractérisé par un vecteur de features $\mathbf{f}_i$ incluant :
\begin{itemize}
    \item \textbf{Position 3D} : Coordonnées $(x, y, z)$ du centroïde
    \item \textbf{Morphologie} : Volume, sphéricité, excentricité, axes principaux
    \item \textbf{Intensités} : Intensités moyennes des canaux fluorescents
    \item \textbf{Texture} : Variance d'intensité, entropie locale
\end{itemize}

\subsection{Stratégies de connectivité}

Plusieurs stratégies de construction d'arêtes ont été comparées :
\begin{itemize}
    \item \textbf{K-nearest neighbors (K-NN)} : Chaque nœud connecté à ses $k$ plus proches voisins
    \item \textbf{Rayon fixe} : Arête si distance $< r$
    \item \textbf{Triangulation de Delaunay} : Connexions géométriques naturelles
    \item \textbf{Hybride} : Combinaison K-NN + seuil de distance maximale
\end{itemize}

\subsection{Normalisation et standardisation des features}

Les features sont normalisées (z-score ou min-max) pour assurer une contribution équilibrée de chaque type d'information et faciliter l'apprentissage.

\subsection{Analyse de sensibilité aux hyperparamètres}

Une étude systématique évalue l'impact des paramètres de construction (valeur de $k$, rayon de connectivité) sur les performances en aval.

\section{Génération de données synthétiques}

\subsection{Motivation : rareté et coût des annotations}

L'annotation manuelle d'organoïdes est coûteuse en temps expert (15-30 min par organoïde) et sujette à subjectivité. La génération de données synthétiques avec labels parfaits permet :
\begin{itemize}
    \item Pré-entraînement de modèles sur grandes quantités de données
    \item Exploration de scénarios rares ou extrêmes
    \item Validation contrôlée des architectures
\end{itemize}

\subsection{Processus ponctuels sur la sphère}

Nous générons des distributions de points sur une sphère (mimant la surface d'un organoïde) selon différents processus stochastiques :

\subsubsection{Processus de Poisson homogène}
Distribution aléatoire complète, référence de hasard complet : $\lambda$ constant.

\subsubsection{Processus de Poisson inhomogène}
Intensité variable spatialement : $\lambda(\mathbf{x})$ fonction de la position, modélise des gradients biologiques.

\subsubsection{Processus de Matérn (clustering)}
Génère des clusters de points, mimant l'agrégation cellulaire observée dans certains phénotypes.

\subsubsection{Processus de Strauss (répulsion)}
Impose une répulsion entre points (distance minimale), représente l'exclusion stérique entre cellules.

\subsection{Diagrammes de Voronoï 3D pour géométrie réaliste}

À partir des centroides générés, nous construisons des cellules de Voronoï 3D pour créer des segmentations réalistes avec volumes et formes variables, reproduisant l'aspect visuel d'organoïdes réels.

\subsection{Simulation de phénotypes contrôlés}

En variant systématiquement les paramètres des processus (intensité, clustering, répulsion), nous simulons différentes classes de phénotypes aux propriétés statistiques contrôlées et connues.

\subsection{Validation statistique}

Les distributions synthétiques sont validées en comparant leurs fonctions K, F, G aux valeurs théoriques attendues et aux distributions observées sur données réelles.

\subsection{Augmentation par perturbations contrôlées}

Des perturbations géométriques (jitter spatial, déformations) et photométriques (variation d'intensités) sont appliquées pour augmenter la diversité et améliorer la robustesse.

\section{Architectures GNN implémentées}

\subsection{Choix d'architectures adaptées aux graphes géométriques}

Nous avons implémenté plusieurs architectures pour évaluer l'importance de l'équivariance géométrique et comparer différentes stratégies d'agrégation.

\subsection{Baseline : GCN, GAT pour ablation}

Les architectures standard (GCN, GAT) servent de baselines pour quantifier l'apport des architectures géométriques. Elles ignorent les coordonnées spatiales et ne respectent pas l'équivariance E(3).

\subsection{EGNN pour équivariance E(3)}

L'architecture EGNN constitue notre modèle principal, exploitant pleinement les coordonnées 3D tout en garantissant l'équivariance aux transformations eucliennes.

\subsection{Adaptations et modifications proposées}

Nous proposons des adaptations spécifiques à notre domaine :
\begin{itemize}
    \item Incorporation de features multi-échelles (locale et globale)
    \item Mécanismes d'attention géométrique
    \item Pooling hiérarchique adapté aux structures sphériques
\end{itemize}

\section{Entraînement et optimisation}

\subsection{Fonctions de perte pour classification}

Pour la classification de graphes, nous utilisons la cross-entropy :
\[
\mathcal{L} = -\sum_{c=1}^C y_c \log(\hat{y}_c)
\]
avec possibilité de pondération pour gérer le déséquilibre de classes.

\subsection{Stratégies de régularisation}

Pour éviter le sur-apprentissage sur les petits datasets :
\begin{itemize}
    \item Dropout sur nœuds et arêtes
    \item Régularisation L2 des poids
    \item Early stopping basé sur validation set
    \item Augmentation de données (rotations, translations)
\end{itemize}

\subsection{Hyperparamètres et recherche automatique}

Les hyperparamètres principaux (learning rate, nombre de couches, dimension cachée, dropout) sont optimisés par recherche en grille ou random search avec validation croisée.

\subsection{Validation croisée adaptée aux petits datasets}

Étant donné la taille limitée des datasets annotés, nous appliquons une validation croisée stratifiée k-fold en veillant à maintenir la distribution des classes.
