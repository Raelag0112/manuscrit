% !TEX root = ../sommaire.tex

\chapter{Méthodologie et pipeline de traitement}

Ce chapitre décrit en détail la méthodologie proposée pour l'analyse automatisée d'organoïdes 3D via Graph Neural Networks. Nous présentons le pipeline complet depuis l'acquisition d'images jusqu'à la prédiction de phénotypes, en justifiant les choix effectués à chaque étape.

\section{Architecture générale du pipeline}

\subsection{Vue d'ensemble}

Notre pipeline transforme des images 3D brutes d'organoïdes en prédictions de phénotypes via une séquence d'étapes automatisées (Figure~[À compléter]). Le flux de données est le suivant :

\begin{enumerate}
    \item \textbf{Input} : Image 3D multicanal ($\sim$2 Go, 2048×2048×200 voxels, 3-4 canaux)
    \item \textbf{Prétraitement} : Normalisation, débruitage, correction → Image nettoyée
    \item \textbf{Segmentation} : Cellpose → Masques de segmentation (labels par cellule)
    \item \textbf{Extraction features} : Calcul propriétés → Table de features cellulaires
    \item \textbf{Clustering spatial} : DBSCAN → Séparation des organoïdes individuels
    \item \textbf{Construction graphes} : K-NN → Graphes géométriques ($\sim$10 Mo)
    \item \textbf{Classification GNN} : EGNN → Prédiction de phénotype + explications
    \item \textbf{Output} : Labels prédits, scores de confiance, cellules importantes
\end{enumerate}

Cette pipeline réduit la dimensionnalité de ~200× (Go → Mo) tout en préservant l'information structurelle biologiquement pertinente.

\subsection{Choix de conception et compromis}

Chaque étape résulte de choix motivés par des contraintes scientifiques, techniques et pratiques.

\subsubsection{Segmentation instance-based vs semantic}

\textbf{Choix} : Segmentation d'instances (cellules individuelles) plutôt que sémantique (type cellulaire).

\textbf{Justification} : Les graphes nécessitent des nœuds discrets. L'identité individuelle des cellules est cruciale pour modéliser les relations. La segmentation sémantique ne fournirait pas cette granularité.

\subsubsection{Représentation graphe vs image brute}

\textbf{Choix} : Abstraction en graphe plutôt que traitement d'image directe.

\textbf{Avantages} :
\begin{itemize}
    \item Compression : réduction mémoire 100-200×
    \item Expressivité : capture explicite des relations cellulaires
    \item Invariances : naturelles aux transformations géométriques
    \item Interprétabilité : identification cellulaire possible
\end{itemize}

\textbf{Compromis} :
\begin{itemize}
    \item Dépendance à la qualité de segmentation
    \item Perte d'information sub-cellulaire (texture intra-cellulaire)
    \item Étape de construction de graphe à paramétrer
\end{itemize}

\subsubsection{EGNN vs GNN standard}

\textbf{Choix} : Architectures équivariantes E(3).

\textbf{Justification} : Les organoïdes n'ont pas d'orientation absolue privilégiée. L'équivariance garantit robustesse et data efficiency. Les études d'ablation (Chapitre 5) quantifieront le gain.

\subsection{Considérations pratiques}

\subsubsection{Temps d'exécution}

Temps typiques par organoïde (CPU : Intel Xeon, GPU : NVIDIA V100) :
\begin{itemize}
    \item Prétraitement : 2-5 sec (CPU)
    \item Segmentation Cellpose : 30-60 sec (GPU)
    \item Extraction features : 1-2 sec (CPU)
    \item Construction graphe : < 1 sec (CPU)
    \item Inférence GNN : < 0.1 sec (GPU, batch)
    \item \textbf{Total : $\sim$1 min/organoïde}
\end{itemize}

Soit 50-100× plus rapide que l'analyse manuelle experte (15-30 min).

\subsubsection{Scalabilité}

Le pipeline est parallélisable :
\begin{itemize}
    \item Chaque organoïde traité indépendamment
    \item Batching de graphes pour inférence GNN
    \item Déploiement sur cluster de calcul possible
\end{itemize}

Pour 1000 organoïdes : $\sim$17 heures sur une seule GPU, ou $\sim$1 heure sur 20 GPUs en parallèle.

\section{Acquisition et prétraitement des images}

\subsection{Protocoles expérimentaux}

\subsubsection{Culture des organoïdes}

\textbf{Conditions standard :}
\begin{itemize}
    \item Matrice : Matrigel (Corning) ou équivalent
    \item Milieu : Milieu de croissance supplémenté (facteurs de croissance spécifiques au type d'organoïde)
    \item Température : 37°C, 5\% CO₂
    \item Renouvellement milieu : tous les 2-3 jours
    \item Passage : tous les 7-14 jours (dissociation mécanique/enzymatique)
\end{itemize}

\textbf{Timing d'imagerie :}
Les organoïdes sont imagés à des jours spécifiques post-passage (typiquement J7-J14) pour assurer une maturité comparable.

\subsubsection{Marquage fluorescent}

\textbf{Fixation et perméabilisation :}
\begin{itemize}
    \item Fixation : Paraformaldéhyde 4\%, 30 min, température ambiante
    \item Perméabilisation : Triton X-100 0.5\%, 30 min
    \item Blocage : BSA 3\%, 1h
\end{itemize}

\textbf{Immunofluorescence :}
\begin{itemize}
    \item Anticorps primaire : overnight, 4°C
    \item Anticorps secondaire conjugué : 2h, température ambiante
    \item Marquage nucléaire : DAPI 1 μg/mL, 10 min
\end{itemize}

\textbf{Canaux typiques :}
\begin{enumerate}
    \item DAPI (noyaux) : 405 nm
    \item Marqueur 1 (\eg Ki67, prolifération) : 488 nm
    \item Marqueur 2 (\eg Caspase-3, apoptose) : 561 nm
    \item Marqueur 3 (\eg lignage-spécifique) : 640 nm
\end{enumerate}

\subsubsection{Paramètres d'acquisition}

\textbf{Microscope confocal :}
\begin{itemize}
    \item Objectif : 40× air (NA 0.95) ou 63× oil (NA 1.4)
    \item Taille voxel : 0.2-0.4 μm (XY), 0.5-1 μm (Z)
    \item Taille image : 2048×2048 pixels (FOV $\sim$ 200-400 μm)
    \item Nombre de Z-slices : 100-300 (selon épaisseur organoïde)
    \item Temps d'acquisition : 2-5 min par organoïde
\end{itemize}

\subsection{Normalisation d'intensité}

Les intensités brutes varient selon les conditions d'acquisition (puissance laser, gain PMT, efficacité de marquage). Une normalisation est cruciale pour la robustesse.

\subsubsection{Normalisation par percentiles}

Plutôt qu'une normalisation min-max sensible aux outliers :
\[
I_{\text{norm}} = \frac{I - P_1}{P_{99} - P_1}
\]

où $P_1$ et $P_{99}$ sont les 1er et 99e percentiles de l'intensité. Cette méthode est robuste aux pixels aberrants (hot pixels).

\subsubsection{Normalisation adaptative par canal}

Chaque canal fluorescent est normalisé indépendamment car :
\begin{itemize}
    \item Les gammes dynamiques diffèrent entre fluorophores
    \item L'efficacité de marquage varie entre anticorps
    \item Les niveaux de fond diffèrent
\end{itemize}

\subsubsection{Correction de fond}

Le fond non-uniforme (autofluorescence, lumière diffusée) est estimé par :
\begin{itemize}
    \item Filtrage morphologique (opening avec large structuring element)
    \item Ou ajustement polynomial de surface
\end{itemize}

puis soustrait : $I_{\text{corr}} = I - I_{\text{fond}}$.

\subsection{Débruitage}

\subsubsection{Sources de bruit}

\begin{itemize}
    \item \textbf{Bruit photonique} : Fluctuations quantiques de photons (Poisson)
    \item \textbf{Bruit de lecture} : Électronique du détecteur (Gaussien)
    \item \textbf{Bruit de fond} : Autofluorescence, lumière ambiante
\end{itemize}

\subsubsection{Filtrage médian 3D}

Le filtre médian remplace chaque voxel par la médiane de son voisinage 3D. Excellent pour réduire le bruit impulsionnel (salt-and-pepper) tout en préservant les arêtes.

Paramètre : taille du noyau (typiquement 3×3×3 ou 5×5×3).

\subsubsection{Filtrage gaussien}

Convolution avec noyau gaussien 3D :
\[
I_{\text{filt}}(\mathbf{x}) = \int I(\mathbf{y}) \mathcal{N}(\mathbf{y}; \mathbf{x}, \sigma^2 \mathbf{I}) d\mathbf{y}
\]

Efficace pour bruit gaussien mais lisse également les structures fines. Compromis via choix de $\sigma$.

\subsubsection{Filtrage bilatéral}

Préserve les arêtes en pondérant la convolution par similarité d'intensité :
\[
I_{\text{bil}}(\mathbf{x}) = \frac{1}{W}\sum_{\mathbf{y} \in \Omega} I(\mathbf{y}) \exp\left(-\frac{\|\mathbf{x}-\mathbf{y}\|^2}{2\sigma_s^2}\right) \exp\left(-\frac{(I(\mathbf{x})-I(\mathbf{y}))^2}{2\sigma_r^2}\right)
\]

Efficace mais lent en 3D. Implémentations GPU accélérées disponibles.

\subsubsection{Choix pour notre pipeline}

Nous appliquons séquentiellement :
\begin{enumerate}
    \item Filtre médian 3×3×3 (bruit impulsionnel)
    \item Filtre gaussien léger (σ = 0.5 voxels) pour bruit résiduel
\end{enumerate}

Ce compromis préserve les détails cellulaires tout en améliorant le SNR de ~3-5 dB.

\subsection{Correction d'artefacts spécifiques}

\subsubsection{Atténuation en profondeur}

L'intensité décroît exponentiellement avec la profondeur $z$ :
\[
I(z) = I_0 \exp(-\mu z)
\]

où $\mu$ est le coefficient d'atténuation.

\textbf{Correction :}
Estimer $\mu$ par régression sur profils d'intensité moyens, puis appliquer :
\[
I_{\text{corr}}(z) = I(z) \cdot \exp(\hat{\mu} z)
\]

\subsubsection{Aberration chromatique}

Les différents canaux peuvent être désalignés (shift XY, Z) du fait de la dispersion chromatique. Correction par :
\begin{enumerate}
    \item Imagerie de billes fluorescentes multi-couleurs (calibration)
    \item Calcul des transformations de recalage (translation, voire affine)
    \item Application aux images d'organoïdes
\end{enumerate}

\section{Segmentation cellulaire automatisée}

La segmentation cellulaire constitue une étape critique, transformant l'image brute en objets discrets (cellules) analysables individuellement.

\subsection{Revue et comparaison des méthodes}

Nous avons évalué systématiquement plusieurs méthodes de segmentation sur un sous-ensemble annoté manuellement de 50 organoïdes (~ 3000 cellules).

\subsubsection{Watershed classique}

\textbf{Algorithme :}
\begin{enumerate}
    \item Détection de markers (maxima locaux après filtrage)
    \item Marker-controlled watershed sur gradient morphologique
    \item Post-traitement (suppression petits objets, fusion)
\end{enumerate}

\textbf{Résultats :}
\begin{itemize}
    \item Dice : 0.72 ± 0.08
    \item Détection objets : Précision 0.78, Rappel 0.82, F1 0.80
    \item Temps : 15 sec/organoïde (CPU)
\end{itemize}

\textbf{Analyse :}
Sur-segmentation fréquente (faux positifs), nécessite tuning manuel des seuils par dataset. Moins robuste que les approches deep learning.

\subsubsection{StarDist}

\textbf{Principe :}
Détecte les centroides cellulaires puis prédit des distances radiales dans 96 directions uniformément distribuées, définissant un polyèdre star-convexe.

\textbf{Résultats :}
\begin{itemize}
    \item Dice : 0.81 ± 0.06
    \item Détection objets : Précision 0.84, Rappel 0.86, F1 0.85
    \item Temps : 40 sec/organoïde (GPU)
\end{itemize}

\textbf{Analyse :}
Performant pour noyaux convexes. Échoue pour morphologies irrégulières ou très allongées (non star-convex). Sensible à la densité cellulaire (chevauchements problématiques).

\subsubsection{Cellpose}

\textbf{Principe :}
Prédit un champ de gradients où chaque pixel "pointe" vers le centre de sa cellule. Le suivi de ces gradients (flow tracking) regroupe les pixels en instances.

\textbf{Architecture :}
\begin{itemize}
    \item Encoder-decoder (U-Net-like) avec ResNet backbone
    \item Deux branches de sortie : gradients X et Y (2D) ou X, Y, Z (3D)
    \item Perte : erreur quadratique sur gradients + classificateur cellule/fond
\end{itemize}

\textbf{Résultats :}
\begin{itemize}
    \item Dice : 0.88 ± 0.04
    \item Détection objets : Précision 0.91, Rappel 0.89, F1 0.90
    \item Temps : 50 sec/organoïde (GPU)
\end{itemize}

\textbf{Analyse :}
État de l'art actuel. Robuste aux variations de taille, forme, densité. Modèles pré-entraînés généralisent bien. Possibilité de fine-tuning améliore encore performances.

\subsection{Choix et paramétrage optimal}

\textbf{Méthode retenue} : Cellpose 3D

\textbf{Paramètres optimaux} :
\begin{itemize}
    \item Modèle : cyto2 (pré-entraîné général) ou fine-tuned sur 100 organoïdes annotés
    \item Diamètre : 15-20 pixels (adapté à résolution et taille noyaux)
    \item Flow threshold : 0.4 (sensibilité du flow tracking)
    \item Cellprob threshold : 0.0 (classificateur cellule/fond)
\end{itemize}

\textbf{Fine-tuning :}
Sur 100 organoïdes annotés manuellement (2 heures d'annotation experte via napari), nous effectuons 100 époques de fine-tuning, améliorant :
\begin{itemize}
    \item Dice : 0.88 → 0.92
    \item F1 détection : 0.90 → 0.94
\end{itemize}

\subsection{Validation qualitative}

Une validation qualitative par inspection visuelle avec deux biologistes experts confirme :
\begin{itemize}
    \item 94\% des segmentations jugées excellentes ou bonnes
    \item 5\% acceptables avec erreurs mineures
    \item 1\% inacceptables (nécessitant retraitement)
\end{itemize}

Les cas problématiques surviennent principalement pour :
\begin{itemize}
    \item Organoïdes très denses (> 1000 cellules, chevauchements extrêmes)
    \item Qualité d'image très dégradée (fort bruit, artefacts)
    \item Noyaux très irréguliers (apoptose avancée)
\end{itemize}

\section{Extraction et séparation des organoïdes}

Une image de puits de culture contient typiquement plusieurs organoïdes à séparer.

\subsection{Conversion en nuages de points}

À partir des masques de segmentation, nous extrayons pour chaque cellule :
\begin{itemize}
    \item \textbf{Centroïde} : Position moyenne $(x_c, y_c, z_c) = \frac{1}{|C|}\sum_{(x,y,z) \in C} (x,y,z)$
    \item \textbf{Volume} : $V = |C| \cdot v_{\text{voxel}}$ où $v_{\text{voxel}}$ est le volume d'un voxel
    \item \textbf{Sphéricité} : $\Psi = \frac{\pi^{1/3}(6V)^{2/3}}{S}$ où $S$ est la surface (estimation via marching cubes)
    \item \textbf{Intensités} : Moyenne et écart-type pour chaque canal dans le masque cellulaire
\end{itemize}

Résultat : table de features $(N_{\text{total}} \times D_f)$ où $N_{\text{total}}$ est le nombre total de cellules dans l'image.

\subsection{Clustering spatial par DBSCAN}

\textbf{Algorithme} : DBSCAN (Density-Based Spatial Clustering of Applications with Noise)

\textbf{Principe :}
Groupe les points denses (≥ minPts voisins dans rayon ε) en clusters, marque les points isolés comme bruit.

\textbf{Paramètres} :
\begin{itemize}
    \item $\epsilon$ : 50-100 μm (1-2× diamètre cellulaire typique)
    \item minPts : 10 cellules (organoïdes minimaux)
\end{itemize}

\textbf{Justification de DBSCAN :}
\begin{itemize}
    \item Trouve automatiquement le nombre de clusters (pas besoin de spécifier K comme dans K-means)
    \item Robuste aux clusters de formes irrégulières
    \item Gère le bruit (débris, faux positifs de segmentation)
\end{itemize}

\subsection{Filtrage et validation}

\subsubsection{Critères de filtrage}

Les clusters identifiés sont filtrés selon :
\begin{itemize}
    \item \textbf{Taille minimale} : ≥ 20 cellules (exclure débris, fragments)
    \item \textbf{Taille maximale} : ≤ 5000 cellules (exclure agrégats aberrants)
    \item \textbf{Compacité} : Ratio volume convex hull / volume réel > 0.7
    \item \textbf{Centrage} : Rejet si trop proche des bords image (organoïdes coupés)
\end{itemize}

\subsubsection{Statistiques de séparation}

Sur un dataset de 500 images contenant 3-10 organoïdes par image :
\begin{itemize}
    \item Recall : 97\% (organoïdes manqués : très petits ou collés au bord)
    \item Précision : 99\% (faux positifs : agrégats de débris passant les filtres)
    \item Erreurs de fusion : < 1\% (deux organoïdes très proches confondus)
\end{itemize}

La séparation est donc très fiable, minimisant les erreurs propagées en aval.

\section{Construction de graphes géométriques}

Cette étape transforme chaque organoïde (nuage de points avec features) en graphe structuré.

\subsection{Définition des nœuds}

Chaque cellule segmentée devient un nœud $v_i$ du graphe, caractérisé par :

\subsubsection{Position 3D}

Coordonnées du centroïde : $\mathbf{x}_i = (x_i, y_i, z_i) \in \mathbb{R}^3$

\textbf{Normalization :}
Pour invariance à la taille absolue de l'organoïde, les coordonnées sont centrées et normalisées :
\[
\mathbf{x}_i' = \frac{\mathbf{x}_i - \bar{\mathbf{x}}}{\text{std}(\mathbf{x})}
\]

où $\bar{\mathbf{x}}$ est le centroïde de l'organoïde.

\subsubsection{Features morphologiques}

\textbf{Géométrie de base :}
\begin{itemize}
    \item Volume : $V_i$ (μm³)
    \item Sphéricité : $\Psi_i \in [0,1]$ (1 = sphère parfaite)
    \item Excentricité : $e_i = \sqrt{1 - \frac{\lambda_3}{\lambda_1}}$ où $\lambda_1 \geq \lambda_2 \geq \lambda_3$ sont les valeurs propres de la matrice d'inertie
\end{itemize}

\textbf{Forme détaillée :}
\begin{itemize}
    \item Axes principaux : $\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3$ (vecteurs propres de la matrice d'inertie)
    \item Élongation : $\lambda_1 / \lambda_2$
    \item Aplatissement : $\lambda_2 / \lambda_3$
    \item Moments d'ordre supérieur : skewness, kurtosis de la distribution de masse
\end{itemize}

\textbf{Surface :}
\begin{itemize}
    \item Surface : $S_i$ (μm²) via algorithme marching cubes
    \item Rugosité : ratio surface réelle / surface sphère équivalente
\end{itemize}

\subsubsection{Features d'intensité}

Pour chaque canal fluorescent $c$ :
\begin{itemize}
    \item Intensité moyenne : $\bar{I}_i^c$
    \item Intensité médiane : $\tilde{I}_i^c$ (robuste aux outliers)
    \item Écart-type : $\sigma_i^c$ (hétérogénéité intra-cellulaire)
    \item Intensité maximale : $I_{\max,i}^c$
    \item Quantiles : $I_{25}^c$, $I_{75}^c$
\end{itemize}

\textbf{Ratios de canaux :}
Pour cellules multi-marquées :
\[
R_{ij} = \frac{\bar{I}_i^{\text{canal1}}}{\bar{I}_i^{\text{canal2}} + \epsilon}
\]

Ces ratios capturent les co-expressions, signatures de types cellulaires.

\subsubsection{Features de texture}

\textbf{Entropie locale :}
\[
H_i = -\sum_k p_k \log p_k
\]

où $p_k$ est l'histogramme normalisé d'intensités dans le masque cellulaire. Mesure la complexité/hétérogénéité de texture.

\textbf{Haralick local :}
Contraste, corrélation calculés sur la matrice de co-occurrence locale.

\subsubsection{Vecteur de features final}

Le vecteur de features d'un nœud est :
\[
\mathbf{f}_i = [\text{position (3)}, \text{morphologie (8)}, \text{intensités (12)}, \text{texture (4)}]^T \in \mathbb{R}^{27}
\]

\textbf{Normalisation :}
Chaque type de feature est z-score normalisé sur le dataset d'entraînement :
\[
f'_{ij} = \frac{f_{ij} - \mu_j}{\sigma_j}
\]

pour assurer contribution équilibrée et faciliter l'apprentissage.

\subsection{Stratégies de connectivité}

La construction des arêtes définit le voisinage et structure le graphe.

\subsubsection{K-Nearest Neighbors (K-NN)}

Connecter chaque nœud à ses $k$ plus proches voisins selon distance euclidienne des centroides.

\textbf{Avantages :}
\begin{itemize}
    \item Degré contrôlé : chaque nœud a exactement $k$ arêtes sortantes
    \item Adaptatif à la densité locale
    \item Graphe connexe (si $k \geq 1$ et organoïde connexe)
\end{itemize}

\textbf{Inconvénients :}
\begin{itemize}
    \item Graphe dirigé asymétrique (i voisin de j $\not\Rightarrow$ j voisin de i)
    \item Symmétrisation nécessaire (union ou intersection des arêtes)
\end{itemize}

\textbf{Choix de k :}
Étude de sensibilité (Section 5.3.3) sur $k \in \{5, 8, 10, 12, 15, 20\}$. Optimal : $k = 10$ pour nos données.

\subsubsection{Rayon fixe (ε-radius)}

Connecter deux nœuds si distance $< r$.

\textbf{Avantages :}
\begin{itemize}
    \item Graphe non-orienté par construction
    \item Interprétation géométrique claire (sphère d'influence)
    \item Signification biologique (portée interactions paracrines ~50 μm)
\end{itemize}

\textbf{Inconvénients :}
\begin{itemize}
    \item Degrés très variables (0 à 50+) selon densité locale
    \item Sensibilité au choix de $r$
    \item Graphe peut être déconnecté si $r$ trop petit
\end{itemize}

\textbf{Choix de r :}
Fixé à 1.5× distance moyenne au plus proche voisin dans le dataset.

\subsubsection{Triangulation de Delaunay}

Tétraédrisation de Delaunay en 3D, connectant naturellement les voisins géométriques.

\textbf{Avantages :}
\begin{itemize}
    \item Construction canonique (pas de paramètre arbitraire)
    \item Propriétés géométriques élégantes
\end{itemize}

\textbf{Inconvénients :}
\begin{itemize}
    \item Peut connecter des cellules très éloignées (tétraèdres allongés à la périphérie)
    \item Coût computationnel plus élevé ($\mathcal{O}(N \log N)$ vs $\mathcal{O}(N k \log N)$ pour K-NN)
\end{itemize}

\subsubsection{Stratégie hybride retenue}

Nous adoptons une approche hybride :
\begin{enumerate}
    \item Construire graphe K-NN avec $k = 10$
    \item Sym métriser : ajouter arête $(j,i)$ si $(i,j)$ existe
    \item Filtrer : supprimer arêtes de longueur > $r_{\max}$ (rejet connexions aberrantes)
\end{enumerate}

Cette stratégie combine les avantages du K-NN (degré contrôlé) et du rayon (rejet arêtes non-physiques).

\subsection{Features d'arêtes}

Chaque arête $(v_i, v_j)$ est enrichie de features optionnelles :

\begin{itemize}
    \item \textbf{Distance euclidienne} : $d_{ij} = \|\mathbf{x}_i - \mathbf{x}_j\|$
    \item \textbf{Vecteur directionnel} : $\mathbf{u}_{ij} = \frac{\mathbf{x}_j - \mathbf{x}_i}{\|\mathbf{x}_j - \mathbf{x}_i\|}$ (vecteur unitaire)
    \item \textbf{Similarité de features} : Distance L2 ou cosine entre $\mathbf{f}_i$ et $\mathbf{f}_j$
\end{itemize}

Pour EGNN, seule la distance est utilisée (équivariance). Pour GNN standards, toutes les features peuvent être exploitées.

\subsection{Analyse de sensibilité}

Une étude systématique (Chapitre 5) évalue l'impact des choix de construction :
\begin{itemize}
    \item Stratégie de connectivité : K-NN vs rayon vs Delaunay vs hybride
    \item Valeur de $k$ : {5, 8, 10, 12, 15, 20}
    \item Valeur de $r$ : {30, 50, 75, 100} μm
    \item Normalisation des coordonnées : avec vs sans
    \item Sous-ensembles de features : ablation de features morpho, intensité, texture
\end{itemize}

Cette analyse guide les choix finaux et révèle les facteurs critiques.

\section{Génération de données synthétiques}

La génération de données synthétiques constitue une contribution méthodologique majeure de cette thèse, adressant le problème critique de rareté d'annotations.

\subsection{Motivation et objectifs}

\subsubsection{Limites des approches classiques}

\textbf{Augmentation de données standard :}
Les techniques d'augmentation classiques (rotations, flips, élasticité, déformations, variations photométriques) génèrent des variations d'échantillons existants mais ne créent pas de nouvelles structures fondamentalement différentes. Elles ne permettent pas d'explorer l'espace complet des phénotypes possibles.

\textbf{GANs et modèles génératifs d'images :}
Les GANs peuvent générer des images d'organoïdes synthétiques. Cependant :
\begin{itemize}
    \item Nécessitent déjà de larges datasets d'entraînement (problème chicken-and-egg)
    \item Génèrent des images pixel-level, pas de segmentations ou labels cellulaires
    \item Labels phénotypiques ambigus ou absence de contrôle fin
    \item Difficile de valider le réalisme biologique
\end{itemize}

\subsubsection{Notre approche : génération contrôlée et validable}

Nous proposons de générer des organoïdes synthétiques via un processus en deux étapes :
\begin{enumerate}
    \item \textbf{Distribution spatiale} : Processus ponctuel stochastique sur sphère → positions cellulaires
    \item \textbf{Géométrisation} : Diagramme de Voronoï 3D → formes cellulaires réalistes
\end{enumerate}

\textbf{Avantages clés :}
\begin{itemize}
    \item \textbf{Contrôle fin} : Paramètres des processus ponctuels contrôlent directement les propriétés statistiques
    \item \textbf{Labels parfaits} : La classe (type de processus) est connue par construction
    \item \textbf{Segmentation parfaite} : Pas d'erreurs de segmentation dans synthétiques
    \item \textbf{Validation statistique rigoureuse} : Fonctions K, F, G comparables aux valeurs théoriques
    \item \textbf{Génération illimitée} : Pas de limite au nombre d'échantillons générables
\end{itemize}

\subsection{Processus ponctuels implémentés}

Nous implémentons cinq types de processus, formant cinq classes pour nos expériences de classification.

\subsubsection{Classe 1 : Poisson homogène (CSR)}

\textbf{Paramètres :} Intensité $\lambda = 0.01$ points/μm² (densité moyenne observée)

\textbf{Interprétation :} Distribution aléatoire complète, référence de hasard. Modélise des organoïdes "normaux" sans organisation spatiale particulière.

\textbf{Simulation :}
\begin{enumerate}
    \item $N \sim \text{Poisson}(4\pi R^2 \lambda)$ avec $R$ rayon sphère (typiquement 100-200 μm)
    \item Positions uniformes sur $\mathbb{S}^2$ (sphère de rayon $R$)
\end{enumerate}

\subsubsection{Classe 2 : Matérn high clustering}

\textbf{Paramètres :} $\lambda_{\text{parent}} = 0.001$, $\lambda_{\text{cluster}} = 0.05$, $r = 30$ μm

\textbf{Interprétation :} Forte agrégation cellulaire. Modélise des organoïdes avec niches locales de prolifération, formations de clusters denses correspondant à des zones actives.

\textbf{Simulation :}
\begin{enumerate}
    \item Générer $N_c \sim \text{Poisson}(\lambda_{\text{parent}} \cdot 4\pi R^2)$ centres
    \item Pour chaque centre, générer $N_k \sim \text{Poisson}(\lambda_{\text{cluster}} \cdot 4\pi r^2)$ points dans rayon $r$ (calotte sphérique)
\end{enumerate}

\subsubsection{Classe 3 : Matérn low clustering}

\textbf{Paramètres :} $\lambda_{\text{parent}} = 0.005$, $\lambda_{\text{cluster}} = 0.02$, $r = 20$ μm

\textbf{Interprétation :} Clustering modéré. Phénotype intermédiaire.

\subsubsection{Classe 4 : Strauss high repulsion}

\textbf{Paramètres :} $\beta = 0.01$, $\gamma = 0.1$, $r = 20$ μm

\textbf{Interprétation :} Forte répulsion entre cellules. Modélise des épithéliums réguliers où les cellules maintiennent un espacement minimal (exclusion stérique, pression osmotique).

\textbf{Simulation :}
MCMC (Metropolis-Hastings) avec $10^5$ itérations burn-in, puis échantillonnage toutes les 100 itérations.

\subsubsection{Classe 5 : Strauss low repulsion}

\textbf{Paramètres :} $\beta = 0.01$, $\gamma = 0.5$, $r = 15$ μm

\textbf{Interprétation :} Répulsion modérée. Régularité spatiale partielle.

\subsection{Construction de Voronoï 3D}

À partir des centroides générés, nous construisons une géométrie réaliste.

\subsubsection{Diagramme de Voronoï}

Le diagramme de Voronoï partitionne l'espace. La cellule de Voronoï de $\mathbf{x}_i$ est :
\[
V_i = \{\mathbf{x} \in \mathbb{R}^3 : \|\mathbf{x} - \mathbf{x}_i\| \leq \|\mathbf{x} - \mathbf{x}_j\| \, \forall j \neq i\}
\]

Chaque cellule de Voronoï est un polyèdre convexe.

\subsubsection{Voronoï sur sphère}

Pour processus sur $\mathbb{S}^2$, le Voronoï est calculé en distance géodésique, partitionnant la surface sphérique en régions.

Pour obtenir un volume 3D :
\begin{enumerate}
    \item Calculer Voronoï 2D sur la surface projetée
    \item Extruder radialement vers l'intérieur (épaisseur $\sim$10-20 μm)
\end{enumerate}

Cela crée des cellules de formes réalistes, allongées radialement comme dans des épithéliums sphériques réels.

\subsubsection{Assignation de propriétés}

Pour chaque cellule de Voronoï :
\begin{itemize}
    \item \textbf{Volume} : Calculé géométriquement
    \item \textbf{Forme} : Sphéricité, excentricité calculées à partir du polyèdre
    \item \textbf{Intensités} : Tirées de distributions calibrées sur données réelles
    \begin{itemize}
        \item Mean intensité : $\mathcal{N}(\mu_{\text{real}}, \sigma_{\text{real}})$
        \item Avec corrélations entre canaux (matrice de covariance estimée)
    \end{itemize}
\end{itemize}

\subsection{Validation statistique des synthétiques}

\subsubsection{Fonctions de Ripley}

Nous calculons K, F, G pour chaque processus simulé et comparons :
\begin{enumerate}
    \item Aux valeurs théoriques attendues
    \item Aux enveloppes de confiance (Monte Carlo)
    \item Aux fonctions observées sur données réelles
\end{enumerate}

\textbf{Critères de validation :}
\begin{itemize}
    \item $K_{\text{simulé}}$ reste dans enveloppes théoriques (99\%)
    \item Patterns qualitatifs (clustering/régularité) visuellement corrects
    \item Distributions de distances inter-cellulaires comparables au réel
\end{itemize}

\subsubsection{Comparaison avec données réelles}

Les distributions des métriques topologiques (degré moyen, clustering, diamètre) des graphes synthétiques sont comparées aux distributions réelles via :
\begin{itemize}
    \item Tests de Kolmogorov-Smirnov (KS test) : $p > 0.05$ acceptation similarité
    \item Visualisation : histogrammes et Q-Q plots
\end{itemize}

\subsection{Stratégies d'augmentation}

Au-delà de la génération de base, nous appliquons des augmentations pour diversifier :

\subsubsection{Perturbations géométriques}

\begin{itemize}
    \item \textbf{Jitter spatial} : Ajouter bruit gaussien aux positions ($\sigma = 2$ μm)
    \item \textbf{Déformation élastique} : Appliquer champ de déformation lisse
    \item \textbf{Variation de rayon} : Échantillonner $R \sim \mathcal{N}(150, 30)$ μm
\end{itemize}

\subsubsection{Perturbations photométriques}

\begin{itemize}
    \item Variation d'intensités : multiplier par facteur aléatoire $\sim \mathcal{N}(1, 0.2)$
    \item Ajout de bruit : Poisson + Gaussien simulant artefacts d'acquisition
    \item Gradients d'atténuation : Simuler perte de signal en profondeur
\end{itemize}

\subsubsection{Dataset synthétique final}

\begin{itemize}
    \item 5 classes × 1000 organoïdes = 5000 organoïdes
    \item Taille moyenne : 250 cellules (range 50-500)
    \item Split : 70\% train (3500), 15\% validation (750), 15\% test (750)
    \item Stockage : ~500 Mo total (graphes compressés)
\end{itemize}

\section{Architectures GNN implémentées}

\subsection{Architecture globale commune}

Toutes nos architectures partagent une structure modulaire :

\textbf{Schéma général :}
\begin{enumerate}
    \item \textbf{Input embedding} : $\mathbf{h}_i^{(0)} = \text{MLP}_{\text{in}}(\mathbf{f}_i)$, projette features initiales dans espace latent
    \item \textbf{Couches de message passing} : $K$ couches transformant $\mathbf{h}_i^{(k-1)} \rightarrow \mathbf{h}_i^{(k)}$
    \item \textbf{Pooling global} : $\mathbf{h}_G = \text{POOL}(\{\mathbf{h}_i^{(K)}\})$, agrège au niveau graphe
    \item \textbf{Classification head} : $\mathbf{y} = \text{MLP}_{\text{out}}(\mathbf{h}_G)$, prédit distribution sur classes
\end{enumerate}

\subsection{GCN baseline}

\textbf{Motivation :}
Référence simple et établie pour évaluer l'apport des architectures plus sophistiquées.

\textbf{Couche GCN :}
\[
\mathbf{h}_i^{(k+1)} = \sigma\left(\sum_{j \in \mathcal{N}(i) \cup \{i\}} \frac{1}{\sqrt{d_i d_j}} \mathbf{W}^{(k)}\mathbf{h}_j^{(k)}\right)
\]

\textbf{Hyperparamètres :}
\begin{itemize}
    \item Nombre de couches : 3
    \item Dimension cachée : 128
    \item Activation : ReLU
    \item Pooling : Global mean pooling
    \item Head : MLP 2 couches (256 → 128 → C classes)
\end{itemize}

\textbf{Nombre de paramètres :} ~250K

\subsection{GAT baseline}

\textbf{Motivation :}
Évaluer l'apport du mécanisme d'attention.

\textbf{Couche GAT multi-head :}
\[
\mathbf{h}_i^{(k+1)} = \|_{m=1}^M \sigma\left(\sum_{j \in \mathcal{N}(i)} \alpha_{ij}^m \mathbf{W}^{m,(k)}\mathbf{h}_j^{(k)}\right)
\]

\textbf{Hyperparamètres :}
\begin{itemize}
    \item Nombre de couches : 3
    \item Têtes d'attention : 4
    \item Dimension cachée : 128 (32 par tête)
    \item Dropout attention : 0.1
    \item Pooling : Global attention-weighted pooling
\end{itemize}

\textbf{Nombre de paramètres :} ~320K

\subsection{EGNN principal}

\textbf{Motivation :}
Architecture principale, exploite pleinement la géométrie 3D et garantit équivariance E(3).

\subsubsection{Couche EGNN}

\textbf{Messages :}
\[
\mathbf{m}_{ij} = \phi_e\left([\mathbf{h}_i \| \mathbf{h}_j \| \|\mathbf{x}_i - \mathbf{x}_j\|^2]\right)
\]

où $\phi_e$ est un MLP (embedding → 128 → 128 → message\_dim).

\textbf{Agrégation :}
\[
\mathbf{m}_i = \sum_{j \in \mathcal{N}(i)} \mathbf{m}_{ij}
\]

\textbf{Mise à jour coordinates :}
\[
\mathbf{x}_i' = \mathbf{x}_i + C \sum_{j \in \mathcal{N}(i)} (\mathbf{x}_i - \mathbf{x}_j) \cdot \phi_x(\mathbf{m}_{ij})
\]

où $\phi_x$ prédit un coefficient scalaire, $C = 1/|\mathcal{N}(i)|$ normalisation.

\textbf{Mise à jour features :}
\[
\mathbf{h}_i' = \phi_h([\mathbf{h}_i \| \mathbf{m}_i])
\]

où $\phi_h$ est un MLP.

\subsubsection{Hyper paramètres}

\begin{itemize}
    \item Nombre de couches : 5 (plus profond que baselines car moins over-smoothing)
    \item Dimension cachée : 256
    \item Message dimension : 128
    \item Activation : SiLU (Swish)
    \item Dropout : 0.15
    \item Normalisation : Layer Norm après chaque couche
    \item Pooling : Mean + Max pooling concaténés
\end{itemize}

\textbf{Nombre de paramètres :} ~800K

\subsubsection{Adaptations spécifiques}

Nous proposons des modifications à l'EGNN standard :

\textbf{Attention géométrique :}
Pondération des messages par fonction de distance :
\[
\mathbf{m}_{ij}' = \mathbf{m}_{ij} \cdot \exp(-d_{ij}^2 / 2\sigma^2)
\]

Les voisins proches contribuent plus que les voisins distants.

\textbf{Multi-scale aggregation :}
Agrégation à différents niveaux de voisinage (1-hop, 2-hop) :
\[
\mathbf{m}_i = \mathbf{m}_i^{(1)} \| \mathbf{m}_i^{(2)}
\]

Capture patterns locaux et plus globaux simultanément.

\subsection{Variante : GNN hiérarchique}

Pour les très grands organoïdes, nous implémentons un pooling hiérarchique.

\textbf{Architecture :}
\begin{enumerate}
    \item EGNN couches 1-3 : message passing au niveau cellulaire
    \item Pooling : Regroupement de cellules en super-nœuds (TopK ou DiffPool)
    \item EGNN couches 4-5 : message passing au niveau super-nœuds
    \item Pooling global : Agrégation finale
\end{enumerate}

\textbf{Avantage :} Réduction de complexité pour organoïdes > 1000 cellules.

\section{Entraînement et optimisation}

\subsection{Fonction de perte}

\subsubsection{Cross-entropy pour classification}

Pour classification multi-classes, nous utilisons la cross-entropy :
\[
\mathcal{L}_{\text{CE}} = -\frac{1}{B}\sum_{b=1}^B \sum_{c=1}^C y_{bc} \log(\hat{y}_{bc})
\]

où $B$ est la taille du batch, $C$ le nombre de classes, $y_{bc}$ le label one-hot, $\hat{y}_{bc}$ la probabilité prédite (après softmax).

\subsubsection{Pondération pour déséquilibre de classes}

En cas de déséquilibre (rare dans notre cas, classes équilibrées), pondération :
\[
\mathcal{L}_{\text{weighted}} = -\frac{1}{B}\sum_{b=1}^B \sum_{c=1}^C w_c \cdot y_{bc} \log(\hat{y}_{bc})
\]

avec $w_c = \frac{N_{\text{total}}}{C \cdot N_c}$ (inverse fréquence).

\subsubsection{Régularisation}

\textbf{L2 weight decay :}
\[
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{CE}} + \lambda_{\text{reg}} \sum_{\ell} \|\mathbf{W}^{(\ell)}\|_F^2
\]

avec $\lambda_{\text{reg}} = 10^{-5}$.

\textbf{Dropout :}
Application de dropout (taux 0.15) sur :
\begin{itemize}
    \item Features de nœuds entre couches
    \item Arêtes (DropEdge) : suppression aléatoire de 10\% arêtes à chaque passe
\end{itemize}

\subsection{Optimisation}

\subsubsection{Algorithme d'optimisation}

\textbf{AdamW :}
Nous utilisons AdamW (Adam avec weight decay découplé) :
\begin{itemize}
    \item Learning rate initial : $10^{-3}$
    \item $\beta_1 = 0.9$, $\beta_2 = 0.999$
    \item Weight decay : $10^{-5}$
    \item Gradient clipping : norm max 1.0
\end{itemize}

\subsubsection{Learning rate scheduling}

\textbf{ReduceLROnPlateau :}
Réduction du learning rate si la métrique de validation stagne :
\begin{itemize}
    \item Facteur : 0.5
    \item Patience : 10 époques
    \item LR minimum : $10^{-6}$
\end{itemize}

Alternativement, \textbf{Cosine annealing} :
\[
\eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})(1 + \cos(\pi t / T))
\]

\subsubsection{Early stopping}

Arrêt si métrique de validation ne s'améliore pas pendant 20 époques consécutives. Restauration des poids de la meilleure époque.

\subsection{Stratégies d'entraînement}

\subsubsection{Entraînement sur synthétiques}

\textbf{Phase 1 : Pré-entraînement}
\begin{itemize}
    \item Dataset : 3500 organoïdes synthétiques (train)
    \item Batch size : 32 graphes
    \item Époques : 200
    \item Augmentation : Rotations aléatoires 3D, jitter, variations d'intensités
    \item Durée : ~2 heures (GPU V100)
\end{itemize}

\subsubsection{Fine-tuning sur réels}

\textbf{Phase 2 : Adaptation au réel}
\begin{itemize}
    \item Initialisation : Poids pré-entraînés (sauf classification head, réinitialisée)
    \item Dataset : N organoïdes réels annotés (typiquement N = 200-500)
    \item Learning rate réduit : $10^{-4}$ (fine-tuning)
    \item Époques : 100
    \item Régularisation accrue : Dropout 0.2, weight decay $10^{-4}$
\end{itemize}

\subsubsection{Entraînement from scratch sur réels}

\textbf{Baseline sans pré-entraînement :}
Pour comparaison, nous entraînons également des modèles directement sur données réelles depuis initialisation aléatoire.

\subsection{Validation croisée}

\subsubsection{Stratégie}

Étant donné les datasets de taille limitée, nous adoptons :

\textbf{5-fold stratified cross-validation :}
\begin{enumerate}
    \item Partitionner dataset en 5 folds stratifiés (distribution de classes préservée)
    \item Pour chaque fold :
    \begin{enumerate}
        \item Entraîner sur 4 folds
        \item Valider sur 1 fold
    \end{enumerate}
    \item Agréger les résultats (moyenne et écart-type des métriques)
\end{enumerate}

\textbf{Nested cross-validation :}
Pour sélection d'hyperparamètres non-biaisée :
\begin{itemize}
    \item Outer loop : 5-fold pour estimation de performance
    \item Inner loop : 3-fold sur train set pour tuning hyperparamètres
\end{itemize}

\subsubsection{Importance du stratification}

La stratification assure que chaque fold contient une proportion représentative de chaque classe, crucial avec petits datasets et classes potentiellement déséquilibrées.

\subsection{Recherche d'hyperparamètres}

\subsubsection{Espace de recherche}

Hyperparamètres explorés :
\begin{itemize}
    \item Nombre de couches : \{3, 4, 5, 6\}
    \item Dimension cachée : \{64, 128, 256, 512\}
    \item Learning rate : \{$10^{-4}$, $5 \times 10^{-4}$, $10^{-3}$, $5 \times 10^{-3}$\}
    \item Dropout : \{0.0, 0.1, 0.15, 0.2, 0.3\}
    \item Batch size : \{16, 32, 64\}
    \item K (connectivité K-NN) : \{5, 8, 10, 12, 15\}
\end{itemize}

\subsubsection{Stratégie de recherche}

\textbf{Random search :}
Plutôt que grid search (combinatoire, coûteux), nous échantillonnons aléatoirement 100 configurations et entraînons chacune pour 50 époques.

\textbf{Critère de sélection :}
Accuracy moyenne sur validation set du inner cross-validation loop.

\subsubsection{Résultats}

\textbf{Configuration optimale trouvée :}
\begin{itemize}
    \item Couches : 5
    \item Dimension : 256
    \item LR : $10^{-3}$
    \item Dropout : 0.15
    \item Batch : 32
    \item K : 10
\end{itemize}

\textbf{Sensibilité :}
Dimension cachée et nombre de couches sont les plus impactants. Learning rate et dropout ont un effet modéré dans les plages explorées.

\section{Implémentation et détails techniques}

\subsection{Stack technologique}

\textbf{Langages et frameworks :}
\begin{itemize}
    \item Python 3.9
    \item PyTorch 2.0 pour deep learning
    \item PyTorch Geometric (PyG) 2.3 pour GNNs
    \item NumPy, SciPy pour calculs scientifiques
    \item scikit-image pour traitement d'image
    \item scikit-learn pour ML classique et métriques
\end{itemize}

\textbf{Segmentation :}
\begin{itemize}
    \item Cellpose 2.2
    \item Interface programmatique (Python API)
\end{itemize}

\textbf{Visualisation :}
\begin{itemize}
    \item Matplotlib, Seaborn pour figures 2D
    \item Plotly pour visualisations 3D interactives
    \item napari pour inspection interactive de segmentations
    \item TensorBoard pour monitoring d'entraînement
\end{itemize}

\subsection{Structures de données}

\subsubsection{Format des graphes}

Nous utilisons le format `torch_geometric.data.Data` :
\begin{itemize}
    \item `x` : Features de nœuds (Tensor de taille $[N, D_f]$)
    \item `edge_index` : Indices d'arêtes (Tensor de taille $[2, |E|]$, format COO)
    \item `edge_attr` : Features d'arêtes (Tensor de taille $[|E|, D_e]$)
    \item `pos` : Coordonnées 3D (Tensor de taille $[N, 3]$)
    \item `y` : Label du graphe (Tensor scalaire ou vecteur)
\end{itemize}

\textbf{Batching :}
PyG gère automatiquement le batching de graphes de tailles différentes via graphe disjoint (union de graphes dans un grand graphe sparse).

\subsubsection{Sauvegarde et chargement}

Les graphes pré-calculés sont sauvegardés en format PyTorch (`.pt`) ou pickle compressé pour accélérer l'entraînement (éviter recalcul de segmentation et graphe construction à chaque run).

\subsection{Optimisations computationnelles}

\subsubsection{Calculs sparse}

Les matrices d'adjacence sont stockées en format sparse (COO ou CSR) pour efficacité mémoire et computationnelle.

\subsubsection{Mixed precision training}

Utilisation de FP16 (float 16 bits) pour :
\begin{itemize}
    \item Réduction mémoire (2×)
    \item Accélération calculs GPU (Tensor Cores)
    \item Maintien de FP32 pour accumulateurs (précision numérique)
\end{itemize}

Via `torch.cuda.amp` (Automatic Mixed Precision).

\subsubsection{Data loading parallèle}

\begin{itemize}
    \item DataLoader avec num\_workers = 4 (threads CPU)
    \item Prefetching : chargement du batch suivant pendant traitement du batch courant
    \item Pin memory pour transfert CPU→GPU accéléré
\end{itemize}

\section{Récapitulatif méthodologique}

Ce chapitre a décrit en détail chaque composante de notre pipeline :

\begin{enumerate}
    \item \textbf{Prétraitement robuste} : Normalisation, débruitage, corrections pour qualité d'image optimale
    \item \textbf{Segmentation state-of-the-art} : Cellpose fine-tuned atteignant 92\% Dice
    \item \textbf{Extraction features riches} : 27 features par cellule (géométrie, morphologie, intensités, texture)
    \item \textbf{Construction graphes réfléchie} : K-NN hybride (k=10) équilibrant connectivité et signification biologique
    \item \textbf{Génération synthétique innovante} : 5 classes de processus ponctuels, validation statistique rigoureuse
    \item \textbf{Architectures GNN adaptées} : EGNN équivariant comme modèle principal, baselines pour ablations
    \item \textbf{Entraînement rigoureux} : Pré-entraînement + fine-tuning, validation croisée, recherche d'hyperparamètres
\end{enumerate}

Le Chapitre 5 présentera les résultats expérimentaux obtenus avec cette méthodologie, validant les choix effectués et quantifiant les performances.
